{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9f7653",
   "metadata": {
    "id": "be9f7653"
   },
   "source": [
    "**Heidelberg University**\n",
    "\n",
    "**Data Science  Group**\n",
    "    \n",
    "Prof. Dr. Michael Gertz  \n",
    "\n",
    "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
    "    \n",
    "October 30, 2023\n",
    "    \n",
    "Natural Language Processing with Transformers\n",
    "\n",
    "Winter Semster 2023/2024     \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e9648",
   "metadata": {
    "id": "258e9648"
   },
   "source": [
    "# **Assignment 1: “Word Embeddings and Probabilistic Language Models”**\n",
    "**Due**: Monday, November 13, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27ad9e",
   "metadata": {
    "id": "fc27ad9e"
   },
   "source": [
    "### **Submission Guidelines**\n",
    "\n",
    "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
    "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
    "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
    "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the complete names of all team members are given in the notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322e8b0",
   "metadata": {
    "id": "e322e8b0"
   },
   "source": [
    "## **Task 1: F.R.I.E.N.D.S and  Word2Vec (Grade (2 + 2 + 4) = 8)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca26ac",
   "metadata": {
    "id": "b4ca26ac"
   },
   "source": [
    "[Friends](https://en.wikipedia.org/wiki/Friends) is an American television sitcom, created by David Crane and Marta Kauffman. In this assignment we will use the transcripts from the show to train a Word2Vec model using the [Gensim](https://radimrehurek.com/gensim/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29cb37",
   "metadata": {
    "id": "fc29cb37"
   },
   "source": [
    "### Subtask 1: Pre-processing\n",
    "We start by loading and cleaning the data. Download the dataset for this assignment and load the `friends_quotes.csv` using pandas. The dataset is from Kaggle (https://www.kaggle.com/ryanstonebraker/friends-transcript) and is created for building a classifier that  determines which friend from the Friend's TV Show would be most likely to say a quote. The column `quote` contains a line from the movie and the `author` is the one who said it. Since these are the only two columns we need, we remove the rest and only keep these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787e2059",
   "metadata": {
    "id": "787e2059"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7e8c7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "7e7e8c7d",
    "outputId": "08e7f5ba-0c89-48bd-d5c7-2fac1f544f24",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monica</td>\n",
       "      <td>Okay, everybody relax. This is not even a date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>Sounds like a date to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>Alright, so I'm back in high school, I'm stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>All</td>\n",
       "      <td>Oh, yeah. Had that dream.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>Then I look down, and I realize there's a phon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                              quote\n",
       "0    Monica  There's nothing to tell! He's just some guy I ...\n",
       "1      Joey  C'mon, you're going out with the guy! There's ...\n",
       "2  Chandler  All right Joey, be nice. So does he have a hum...\n",
       "3    Phoebe                           Wait, does he eat chalk?\n",
       "4    Phoebe  Just, 'cause, I don't want her to go through w...\n",
       "5    Monica  Okay, everybody relax. This is not even a date...\n",
       "6  Chandler                          Sounds like a date to me.\n",
       "7  Chandler  Alright, so I'm back in high school, I'm stand...\n",
       "8       All                          Oh, yeah. Had that dream.\n",
       "9  Chandler  Then I look down, and I realize there's a phon..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_csv(\"friends_quotes.csv\")\n",
    "df.drop(['episode_number','episode_title', 'quote_order','season'], inplace=True, axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fc72f",
   "metadata": {
    "id": "be5fc72f"
   },
   "source": [
    "Fortunately, there is no missing data, so we do not need to worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3aea10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc3aea10",
    "outputId": "36edb926-7922-4d63-ad29-411f889ce920"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author    0\n",
       "quote     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # check for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a249b1",
   "metadata": {
    "id": "26a249b1"
   },
   "source": [
    "Use SpaCy to preprocess the text. For this, perform the following steps:\n",
    "- lowercase the words\n",
    "- remove the stopwords and single characters\n",
    "- use regex to remove non-alphabetic characters (anything that is not a number or alphabet including punctuations), in other words only keep \"a\" to \"z\" and digits.\n",
    "- remove lines that have less than 4 words, since they cannot contribute much to the training process.\n",
    "\n",
    "Please do not add any additional steps on your own or additional cleaning as we want to achieve comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5491d9f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5491d9f0",
    "outputId": "6010f9a2-d5a8-41f6-b04a-3b1341e19d7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushal/.config/python.env/nlptrans.env/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def cleaning(text):\n",
    "    doc= nlp(text)\n",
    "    #remove stopwords or single character and lines that have less than 3 words\n",
    "    cleaned_text = \" \".join(str(token) for token in doc if str(token) not in stopwords and len(doc)>3 and  len(token)>1)\n",
    "    return cleaned_text\n",
    "\n",
    "#df[\"quote\"] = # lowercase and remove non-alphabetic characters\n",
    "df[\"quote\"] = df[\"quote\"].str.lower().replace('[^a-zA-Z]', ' ', regex=True)\n",
    "df[\"quote\"] = df[\"quote\"].apply(lambda x : cleaning(x))\n",
    "\n",
    "quotes =[] # to save all the lines\n",
    "quotes= list(filter(None,df[\"quote\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa2190",
   "metadata": {
    "id": "6aaa2190"
   },
   "source": [
    "The next step is to build the vocabulary of the words and word combinations we want to learn representations from. We choose a subset of the most frequent words and bigrams to represent our corpus.\n",
    "- Use the Gensim Phrases package to automatically detect common phrases (bigrams) from a list of lines from the previous step (`min_count=10`). Now words like New_York will be considered as one entity and character names like joey_tribbiani will be recognized.\n",
    "- Create a list of words/bigrams with their frequencies and choose the top 15.000 words for the vocabulary, in order to keep the computation time-limited and to choose the most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3059fd0",
   "metadata": {
    "id": "a3059fd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:01:25: collecting all words and their counts\n",
      "INFO - 05:01:25: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 05:01:25: PROGRESS: at sentence #10000, processed 73384 words and 44183 word types\n",
      "INFO - 05:01:25: PROGRESS: at sentence #20000, processed 146896 words and 77278 word types\n",
      "INFO - 05:01:25: PROGRESS: at sentence #30000, processed 223085 words and 106430 word types\n",
      "INFO - 05:01:25: PROGRESS: at sentence #40000, processed 297030 words and 133475 word types\n",
      "INFO - 05:01:25: collected 157120 token types (unigram + bigrams) from a corpus of 364939 words and 48537 sentences\n",
      "INFO - 05:01:25: merged Phrases<157120 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 05:01:25: Phrases lifecycle event {'msg': 'built Phrases<157120 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000> in 0.25s', 'datetime': '2023-11-13T05:01:25.973620', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "words = [line.split(' ') for line in quotes] #list of all words#\n",
    "phrases = Phrases(words, min_count=10)#define the phraser for bi-gram creation#\n",
    "new_lines = phrases[words] #transform the lines#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ca7541",
   "metadata": {
    "id": "a9ca7541"
   },
   "outputs": [],
   "source": [
    "### find the top words for the vocabulary###\n",
    "vocab=sorted(phrases.vocab.items(), key = lambda x: x[1], reverse=True)[:15000]###top words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d08031d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d08031d",
    "outputId": "77d80f2d-f20f-4c11-d2b9-b6b9c8097d64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = dict(vocab)\n",
    "\n",
    "word_freq['central_perk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0882dc8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0882dc8c",
    "outputId": "69f86314-727d-417c-9f92-3980fa60ca9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2342"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq['joey']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dfce56",
   "metadata": {
    "id": "15dfce56"
   },
   "source": [
    "### Subtask 2: Training the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c94e0",
   "metadata": {
    "id": "b59c94e0"
   },
   "source": [
    "Use the Gensim implementation of Word2Vec to train a model on the scripts. The training can be divided into 3 stages:\n",
    "\n",
    "\n",
    "1) Set up your model with parameters; define your parameters in such a way that the following conditions are satisfied:\n",
    " - ignore all words that have a total frequency of less than 2.\n",
    " - dimensions of the embeddings: 100\n",
    " - initial learning rate (step size) of 0.03\n",
    " - 20 negative samples\n",
    " - window size 3\n",
    " - the learning rate in the training will decrease as you apply more and more updates. Most of the time when starting with gradient descent the initial steps can be larger, and as we get close to the local minima it is best to use smaller steps to avoid jumping over the local minima. This adjustment is done internally using a learning rate scheduler. Make sure that the smallest learning rate does not go below 0.0001.\n",
    " - set the threshold for configuring which higher-frequency words are randomly down-sampled to 6e-5. This parameter forces the sampling to choose the very frequent words less often in the sampling.\n",
    " - set the hashfunction of the word2vec to the given function.\n",
    " - train on a single worker to make sure you get the same result as ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfbce0a2",
   "metadata": {
    "id": "dfbce0a2"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def hash(astring):\n",
    "    return ord(astring[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c12a7a6",
   "metadata": {
    "id": "7c12a7a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:01:26: collecting all words and their counts\n",
      "INFO - 05:01:26: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 05:01:26: PROGRESS: at sentence #10000, processed 57478 words, keeping 7546 word types\n",
      "INFO - 05:01:26: PROGRESS: at sentence #20000, processed 115638 words, keeping 10398 word types\n",
      "INFO - 05:01:26: PROGRESS: at sentence #30000, processed 174962 words, keeping 12346 word types\n",
      "INFO - 05:01:26: PROGRESS: at sentence #40000, processed 233279 words, keeping 14069 word types\n",
      "INFO - 05:01:26: collected 15818 word types from a corpus of 282772 raw words and 48537 sentences\n",
      "INFO - 05:01:26: Creating a fresh vocabulary\n",
      "INFO - 05:01:26: Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 9351 unique words (59.12% of original 15818, drops 6467)', 'datetime': '2023-11-13T05:01:26.456505', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "INFO - 05:01:26: Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 276305 word corpus (97.71% of original 282772, drops 6467)', 'datetime': '2023-11-13T05:01:26.456968', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "INFO - 05:01:26: deleting the raw counts dictionary of 15818 items\n",
      "INFO - 05:01:26: sample=6e-05 downsamples 935 most-common words\n",
      "INFO - 05:01:26: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 126618.52964569976 word corpus (45.8%% of prior 276305)', 'datetime': '2023-11-13T05:01:26.484078', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "INFO - 05:01:26: estimated required memory for 9351 words and 100 dimensions: 12156300 bytes\n",
      "INFO - 05:01:26: resetting layer weights\n",
      "INFO - 05:01:26: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-13T05:01:26.534069', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'build_vocab'}\n",
      "INFO - 05:01:26: Word2Vec lifecycle event {'msg': 'training model with 1 workers on 9351 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=3 shrink_windows=True', 'datetime': '2023-11-13T05:01:26.534698', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'train'}\n",
      "INFO - 05:01:27: EPOCH 0: training on 282772 raw words (126505 effective words) took 0.5s, 251408 effective words/s\n",
      "INFO - 05:01:27: EPOCH 1: training on 282772 raw words (126478 effective words) took 0.4s, 319321 effective words/s\n",
      "INFO - 05:01:27: EPOCH 2: training on 282772 raw words (126779 effective words) took 0.5s, 272746 effective words/s\n",
      "INFO - 05:01:28: EPOCH 3: training on 282772 raw words (126951 effective words) took 0.4s, 299569 effective words/s\n",
      "INFO - 05:01:28: EPOCH 4: training on 282772 raw words (126656 effective words) took 0.5s, 265708 effective words/s\n",
      "INFO - 05:01:28: Word2Vec lifecycle event {'msg': 'training on 1413860 raw words (633369 effective words) took 2.3s, 275948 effective words/s', 'datetime': '2023-11-13T05:01:28.830313', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'train'}\n",
      "INFO - 05:01:28: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=9351, vector_size=100, alpha=0.03>', 'datetime': '2023-11-13T05:01:28.830650', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    " ### your code ###\n",
    "w2v = Word2Vec(\n",
    "    new_lines,\n",
    "    workers = 1,\n",
    "    vector_size = 100,\n",
    "    min_count = 2,\n",
    "    window = 3,\n",
    "    sample = 6e-5,\n",
    "    negative= 20,\n",
    "    alpha = 0.03,\n",
    "    min_alpha = 0.0001,\n",
    "    hashfxn= hash,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eba781",
   "metadata": {
    "id": "03eba781"
   },
   "source": [
    "2) Before training, Word2Vec requires us to build the vocabulary table by filtering out the unique words and doing some basic counts on them.\n",
    "Use the `build_vocab` function to process the data. If you look at the logs you can see the effect of `min_count` and `sample` on the word corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9795441",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9795441",
    "outputId": "95ba6845-b3b1-4a3a-a5d0-7b38ca96012e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:01:28: collecting all words and their counts\n",
      "INFO - 05:01:28: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 05:01:28: PROGRESS: at sentence #10000, processed 57478 words, keeping 7546 word types\n",
      "INFO - 05:01:28: PROGRESS: at sentence #20000, processed 115638 words, keeping 10398 word types\n",
      "INFO - 05:01:29: PROGRESS: at sentence #30000, processed 174962 words, keeping 12346 word types\n",
      "INFO - 05:01:29: PROGRESS: at sentence #40000, processed 233279 words, keeping 14069 word types\n",
      "INFO - 05:01:29: collected 15818 word types from a corpus of 282772 raw words and 48537 sentences\n",
      "INFO - 05:01:29: Creating a fresh vocabulary\n",
      "INFO - 05:01:29: Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 9351 unique words (59.12% of original 15818, drops 6467)', 'datetime': '2023-11-13T05:01:29.179360', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "INFO - 05:01:29: Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 276305 word corpus (97.71% of original 282772, drops 6467)', 'datetime': '2023-11-13T05:01:29.180009', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "INFO - 05:01:29: deleting the raw counts dictionary of 15818 items\n",
      "INFO - 05:01:29: sample=6e-05 downsamples 935 most-common words\n",
      "INFO - 05:01:29: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 126618.52964569976 word corpus (45.8%% of prior 276305)', 'datetime': '2023-11-13T05:01:29.207213', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'prepare_vocab'}\n",
      "WARNING - 05:01:29: sorting after vectors have been allocated is expensive & error-prone\n",
      "INFO - 05:01:29: estimated required memory for 9351 words and 100 dimensions: 12156300 bytes\n",
      "INFO - 05:01:29: resetting layer weights\n",
      "INFO - 05:01:29: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-13T05:01:29.255541', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "w2v.build_vocab(new_lines) ### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdc06f",
   "metadata": {
    "id": "aefdc06f"
   },
   "source": [
    "3) Finally, we  train the model. Train the model for 100 epochs. This will take a while. As we do not plan to train the model any further, we call `init_sims()`, which will make the model much more memory-efficient by precomputing L2-norms of word weight vectors for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0febaedc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0febaedc",
    "outputId": "a905bc03-eb03-4f33-9e88-a2d8fa738c20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 05:01:29: Effective 'alpha' higher than previous training cycles\n",
      "INFO - 05:01:29: Word2Vec lifecycle event {'msg': 'training model with 1 workers on 9351 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=3 shrink_windows=True', 'datetime': '2023-11-13T05:01:29.259839', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'train'}\n",
      "INFO - 05:01:29: EPOCH 0: training on 282772 raw words (126698 effective words) took 0.5s, 269448 effective words/s\n",
      "WARNING - 05:01:29: EPOCH 0: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:30: EPOCH 1: training on 282772 raw words (126509 effective words) took 0.5s, 254422 effective words/s\n",
      "WARNING - 05:01:30: EPOCH 1: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:30: EPOCH 2: training on 282772 raw words (126687 effective words) took 0.5s, 272021 effective words/s\n",
      "WARNING - 05:01:30: EPOCH 2: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:31: EPOCH 3: training on 282772 raw words (126365 effective words) took 0.4s, 299158 effective words/s\n",
      "WARNING - 05:01:31: EPOCH 3: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:31: EPOCH 4: training on 282772 raw words (126806 effective words) took 0.5s, 241055 effective words/s\n",
      "WARNING - 05:01:31: EPOCH 4: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:32: EPOCH 5: training on 282772 raw words (126344 effective words) took 0.4s, 295000 effective words/s\n",
      "WARNING - 05:01:32: EPOCH 5: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:32: EPOCH 6: training on 282772 raw words (126716 effective words) took 0.5s, 246371 effective words/s\n",
      "WARNING - 05:01:32: EPOCH 6: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:33: EPOCH 7: training on 282772 raw words (126606 effective words) took 0.4s, 299341 effective words/s\n",
      "WARNING - 05:01:33: EPOCH 7: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:33: EPOCH 8: training on 282772 raw words (126767 effective words) took 0.5s, 267761 effective words/s\n",
      "WARNING - 05:01:33: EPOCH 8: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:33: EPOCH 9: training on 282772 raw words (126543 effective words) took 0.4s, 300230 effective words/s\n",
      "WARNING - 05:01:33: EPOCH 9: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:34: EPOCH 10: training on 282772 raw words (126870 effective words) took 0.4s, 296630 effective words/s\n",
      "WARNING - 05:01:34: EPOCH 10: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:34: EPOCH 11: training on 282772 raw words (126606 effective words) took 0.5s, 245034 effective words/s\n",
      "WARNING - 05:01:34: EPOCH 11: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:35: EPOCH 12: training on 282772 raw words (126660 effective words) took 0.4s, 293438 effective words/s\n",
      "WARNING - 05:01:35: EPOCH 12: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:35: EPOCH 13: training on 282772 raw words (126624 effective words) took 0.5s, 237026 effective words/s\n",
      "WARNING - 05:01:35: EPOCH 13: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:36: EPOCH 14: training on 282772 raw words (126801 effective words) took 0.4s, 283702 effective words/s\n",
      "WARNING - 05:01:36: EPOCH 14: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:36: EPOCH 15: training on 282772 raw words (126587 effective words) took 0.4s, 295869 effective words/s\n",
      "WARNING - 05:01:36: EPOCH 15: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:37: EPOCH 16: training on 282772 raw words (126356 effective words) took 0.5s, 262015 effective words/s\n",
      "WARNING - 05:01:37: EPOCH 16: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:37: EPOCH 17: training on 282772 raw words (126498 effective words) took 0.5s, 277926 effective words/s\n",
      "WARNING - 05:01:37: EPOCH 17: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:38: EPOCH 18: training on 282772 raw words (126630 effective words) took 0.5s, 253160 effective words/s\n",
      "WARNING - 05:01:38: EPOCH 18: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:38: EPOCH 19: training on 282772 raw words (126650 effective words) took 0.5s, 272729 effective words/s\n",
      "WARNING - 05:01:38: EPOCH 19: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:39: EPOCH 20: training on 282772 raw words (126597 effective words) took 0.5s, 247574 effective words/s\n",
      "WARNING - 05:01:39: EPOCH 20: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:39: EPOCH 21: training on 282772 raw words (126394 effective words) took 0.5s, 276575 effective words/s\n",
      "WARNING - 05:01:39: EPOCH 21: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:40: EPOCH 22: training on 282772 raw words (126602 effective words) took 0.4s, 299221 effective words/s\n",
      "WARNING - 05:01:40: EPOCH 22: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:40: EPOCH 23: training on 282772 raw words (126653 effective words) took 0.4s, 293084 effective words/s\n",
      "WARNING - 05:01:40: EPOCH 23: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:41: EPOCH 24: training on 282772 raw words (126673 effective words) took 0.4s, 293827 effective words/s\n",
      "WARNING - 05:01:41: EPOCH 24: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:41: EPOCH 25: training on 282772 raw words (126679 effective words) took 0.5s, 253767 effective words/s\n",
      "WARNING - 05:01:41: EPOCH 25: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:42: EPOCH 26: training on 282772 raw words (126661 effective words) took 0.6s, 221581 effective words/s\n",
      "WARNING - 05:01:42: EPOCH 26: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:42: EPOCH 27: training on 282772 raw words (126702 effective words) took 0.5s, 275535 effective words/s\n",
      "WARNING - 05:01:42: EPOCH 27: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:43: EPOCH 28: training on 282772 raw words (126575 effective words) took 0.4s, 287295 effective words/s\n",
      "WARNING - 05:01:43: EPOCH 28: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:43: EPOCH 29: training on 282772 raw words (126591 effective words) took 0.6s, 229784 effective words/s\n",
      "WARNING - 05:01:43: EPOCH 29: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:44: EPOCH 30: training on 282772 raw words (126795 effective words) took 0.4s, 318051 effective words/s\n",
      "WARNING - 05:01:44: EPOCH 30: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:44: EPOCH 31: training on 282772 raw words (126545 effective words) took 0.4s, 286196 effective words/s\n",
      "WARNING - 05:01:44: EPOCH 31: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:44: EPOCH 32: training on 282772 raw words (126252 effective words) took 0.5s, 256448 effective words/s\n",
      "WARNING - 05:01:44: EPOCH 32: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:45: EPOCH 33: training on 282772 raw words (126787 effective words) took 0.4s, 303830 effective words/s\n",
      "WARNING - 05:01:45: EPOCH 33: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:45: EPOCH 34: training on 282772 raw words (126745 effective words) took 0.5s, 265813 effective words/s\n",
      "WARNING - 05:01:45: EPOCH 34: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:46: EPOCH 35: training on 282772 raw words (126546 effective words) took 0.4s, 284579 effective words/s\n",
      "WARNING - 05:01:46: EPOCH 35: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:46: EPOCH 36: training on 282772 raw words (126958 effective words) took 0.4s, 302872 effective words/s\n",
      "WARNING - 05:01:46: EPOCH 36: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:47: EPOCH 37: training on 282772 raw words (126493 effective words) took 0.5s, 269324 effective words/s\n",
      "WARNING - 05:01:47: EPOCH 37: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:47: EPOCH 38: training on 282772 raw words (126645 effective words) took 0.4s, 290869 effective words/s\n",
      "WARNING - 05:01:47: EPOCH 38: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:48: EPOCH 39: training on 282772 raw words (126719 effective words) took 0.5s, 268637 effective words/s\n",
      "WARNING - 05:01:48: EPOCH 39: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:48: EPOCH 40: training on 282772 raw words (126422 effective words) took 0.4s, 291360 effective words/s\n",
      "WARNING - 05:01:48: EPOCH 40: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:49: EPOCH 41: training on 282772 raw words (126539 effective words) took 0.5s, 265016 effective words/s\n",
      "WARNING - 05:01:49: EPOCH 41: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:49: EPOCH 42: training on 282772 raw words (126482 effective words) took 0.4s, 308391 effective words/s\n",
      "WARNING - 05:01:49: EPOCH 42: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:49: EPOCH 43: training on 282772 raw words (126591 effective words) took 0.4s, 298866 effective words/s\n",
      "WARNING - 05:01:49: EPOCH 43: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:50: EPOCH 44: training on 282772 raw words (126806 effective words) took 0.6s, 229385 effective words/s\n",
      "WARNING - 05:01:50: EPOCH 44: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:50: EPOCH 45: training on 282772 raw words (126345 effective words) took 0.4s, 284824 effective words/s\n",
      "WARNING - 05:01:50: EPOCH 45: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:51: EPOCH 46: training on 282772 raw words (126363 effective words) took 0.5s, 258526 effective words/s\n",
      "WARNING - 05:01:51: EPOCH 46: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:51: EPOCH 47: training on 282772 raw words (126873 effective words) took 0.4s, 285823 effective words/s\n",
      "WARNING - 05:01:51: EPOCH 47: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:52: EPOCH 48: training on 282772 raw words (126413 effective words) took 0.4s, 292954 effective words/s\n",
      "WARNING - 05:01:52: EPOCH 48: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:52: EPOCH 49: training on 282772 raw words (126726 effective words) took 0.5s, 259595 effective words/s\n",
      "WARNING - 05:01:52: EPOCH 49: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:53: EPOCH 50: training on 282772 raw words (126573 effective words) took 0.4s, 298663 effective words/s\n",
      "WARNING - 05:01:53: EPOCH 50: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:53: EPOCH 51: training on 282772 raw words (126577 effective words) took 0.5s, 254873 effective words/s\n",
      "WARNING - 05:01:53: EPOCH 51: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:54: EPOCH 52: training on 282772 raw words (126556 effective words) took 0.4s, 312371 effective words/s\n",
      "WARNING - 05:01:54: EPOCH 52: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:54: EPOCH 53: training on 282772 raw words (126537 effective words) took 0.5s, 260051 effective words/s\n",
      "WARNING - 05:01:54: EPOCH 53: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:55: EPOCH 54: training on 282772 raw words (126592 effective words) took 0.5s, 268886 effective words/s\n",
      "WARNING - 05:01:55: EPOCH 54: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:55: EPOCH 55: training on 282772 raw words (126709 effective words) took 0.4s, 305747 effective words/s\n",
      "WARNING - 05:01:55: EPOCH 55: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:56: EPOCH 56: training on 282772 raw words (126768 effective words) took 0.5s, 265306 effective words/s\n",
      "WARNING - 05:01:56: EPOCH 56: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:56: EPOCH 57: training on 282772 raw words (126813 effective words) took 0.4s, 287270 effective words/s\n",
      "WARNING - 05:01:56: EPOCH 57: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:56: EPOCH 58: training on 282772 raw words (126418 effective words) took 0.5s, 261634 effective words/s\n",
      "WARNING - 05:01:56: EPOCH 58: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:57: EPOCH 59: training on 282772 raw words (126775 effective words) took 0.4s, 298511 effective words/s\n",
      "WARNING - 05:01:57: EPOCH 59: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:57: EPOCH 60: training on 282772 raw words (126946 effective words) took 0.4s, 291838 effective words/s\n",
      "WARNING - 05:01:57: EPOCH 60: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:58: EPOCH 61: training on 282772 raw words (126492 effective words) took 0.5s, 256613 effective words/s\n",
      "WARNING - 05:01:58: EPOCH 61: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:58: EPOCH 62: training on 282772 raw words (126559 effective words) took 0.4s, 282128 effective words/s\n",
      "WARNING - 05:01:58: EPOCH 62: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:59: EPOCH 63: training on 282772 raw words (126295 effective words) took 0.5s, 276108 effective words/s\n",
      "WARNING - 05:01:59: EPOCH 63: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:01:59: EPOCH 64: training on 282772 raw words (126425 effective words) took 0.4s, 315444 effective words/s\n",
      "WARNING - 05:01:59: EPOCH 64: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:00: EPOCH 65: training on 282772 raw words (126430 effective words) took 0.5s, 260753 effective words/s\n",
      "WARNING - 05:02:00: EPOCH 65: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:00: EPOCH 66: training on 282772 raw words (126761 effective words) took 0.4s, 302544 effective words/s\n",
      "WARNING - 05:02:00: EPOCH 66: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:01: EPOCH 67: training on 282772 raw words (126456 effective words) took 0.4s, 294395 effective words/s\n",
      "WARNING - 05:02:01: EPOCH 67: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:01: EPOCH 68: training on 282772 raw words (126665 effective words) took 0.5s, 273950 effective words/s\n",
      "WARNING - 05:02:01: EPOCH 68: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:01: EPOCH 69: training on 282772 raw words (126432 effective words) took 0.4s, 294010 effective words/s\n",
      "WARNING - 05:02:01: EPOCH 69: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:02: EPOCH 70: training on 282772 raw words (126468 effective words) took 0.5s, 248702 effective words/s\n",
      "WARNING - 05:02:02: EPOCH 70: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:02: EPOCH 71: training on 282772 raw words (126912 effective words) took 0.5s, 274219 effective words/s\n",
      "WARNING - 05:02:02: EPOCH 71: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:03: EPOCH 72: training on 282772 raw words (126693 effective words) took 0.5s, 261656 effective words/s\n",
      "WARNING - 05:02:03: EPOCH 72: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:03: EPOCH 73: training on 282772 raw words (126504 effective words) took 0.4s, 302689 effective words/s\n",
      "WARNING - 05:02:03: EPOCH 73: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:04: EPOCH 74: training on 282772 raw words (126652 effective words) took 0.4s, 312234 effective words/s\n",
      "WARNING - 05:02:04: EPOCH 74: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:04: EPOCH 75: training on 282772 raw words (126484 effective words) took 0.5s, 259431 effective words/s\n",
      "WARNING - 05:02:04: EPOCH 75: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:05: EPOCH 76: training on 282772 raw words (126675 effective words) took 0.4s, 296320 effective words/s\n",
      "WARNING - 05:02:05: EPOCH 76: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:05: EPOCH 77: training on 282772 raw words (127015 effective words) took 0.5s, 261066 effective words/s\n",
      "WARNING - 05:02:05: EPOCH 77: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:06: EPOCH 78: training on 282772 raw words (126725 effective words) took 0.4s, 302760 effective words/s\n",
      "WARNING - 05:02:06: EPOCH 78: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:06: EPOCH 79: training on 282772 raw words (126497 effective words) took 0.5s, 248454 effective words/s\n",
      "WARNING - 05:02:06: EPOCH 79: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:07: EPOCH 80: training on 282772 raw words (126389 effective words) took 0.4s, 311700 effective words/s\n",
      "WARNING - 05:02:07: EPOCH 80: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:07: EPOCH 81: training on 282772 raw words (126114 effective words) took 0.4s, 314540 effective words/s\n",
      "WARNING - 05:02:07: EPOCH 81: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:07: EPOCH 82: training on 282772 raw words (126627 effective words) took 0.5s, 268604 effective words/s\n",
      "WARNING - 05:02:07: EPOCH 82: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:08: EPOCH 83: training on 282772 raw words (126921 effective words) took 0.4s, 326238 effective words/s\n",
      "WARNING - 05:02:08: EPOCH 83: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:08: EPOCH 84: training on 282772 raw words (126817 effective words) took 0.6s, 225463 effective words/s\n",
      "WARNING - 05:02:08: EPOCH 84: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:09: EPOCH 85: training on 282772 raw words (126625 effective words) took 0.4s, 304574 effective words/s\n",
      "WARNING - 05:02:09: EPOCH 85: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:09: EPOCH 86: training on 282772 raw words (126669 effective words) took 0.5s, 260452 effective words/s\n",
      "WARNING - 05:02:09: EPOCH 86: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:10: EPOCH 87: training on 282772 raw words (126795 effective words) took 0.4s, 307274 effective words/s\n",
      "WARNING - 05:02:10: EPOCH 87: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:10: EPOCH 88: training on 282772 raw words (126408 effective words) took 0.4s, 290970 effective words/s\n",
      "WARNING - 05:02:10: EPOCH 88: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:11: EPOCH 89: training on 282772 raw words (126695 effective words) took 0.5s, 271665 effective words/s\n",
      "WARNING - 05:02:11: EPOCH 89: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:11: EPOCH 90: training on 282772 raw words (126210 effective words) took 0.4s, 300265 effective words/s\n",
      "WARNING - 05:02:11: EPOCH 90: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:12: EPOCH 91: training on 282772 raw words (126563 effective words) took 0.5s, 266084 effective words/s\n",
      "WARNING - 05:02:12: EPOCH 91: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:12: EPOCH 92: training on 282772 raw words (126641 effective words) took 0.4s, 289233 effective words/s\n",
      "WARNING - 05:02:12: EPOCH 92: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:12: EPOCH 93: training on 282772 raw words (126759 effective words) took 0.5s, 261147 effective words/s\n",
      "WARNING - 05:02:12: EPOCH 93: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:13: EPOCH 94: training on 282772 raw words (126629 effective words) took 0.4s, 314815 effective words/s\n",
      "WARNING - 05:02:13: EPOCH 94: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:13: EPOCH 95: training on 282772 raw words (126941 effective words) took 0.4s, 304782 effective words/s\n",
      "WARNING - 05:02:13: EPOCH 95: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:14: EPOCH 96: training on 282772 raw words (126690 effective words) took 0.5s, 265729 effective words/s\n",
      "WARNING - 05:02:14: EPOCH 96: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:14: EPOCH 97: training on 282772 raw words (126644 effective words) took 0.4s, 305233 effective words/s\n",
      "WARNING - 05:02:14: EPOCH 97: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:15: EPOCH 98: training on 282772 raw words (126705 effective words) took 0.5s, 264560 effective words/s\n",
      "WARNING - 05:02:15: EPOCH 98: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:15: EPOCH 99: training on 282772 raw words (126310 effective words) took 0.4s, 323726 effective words/s\n",
      "WARNING - 05:02:15: EPOCH 99: supplied raw word count (282772) did not equal expected count (48537)\n",
      "INFO - 05:02:15: Word2Vec lifecycle event {'msg': 'training on 28277200 raw words (12661051 effective words) took 46.3s, 273168 effective words/s', 'datetime': '2023-11-13T05:02:15.609225', 'gensim': '4.3.2', 'python': '3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]', 'platform': 'Linux-6.6.1-arch1-1-x86_64-with-glibc2.38', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12661051, 28277200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.train(new_lines,total_words= len(new_lines),epochs=100) ### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "WDZhrCk1URL5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDZhrCk1URL5",
    "outputId": "2c6c5e1e-2102-4288-adbf-bc787b9c06c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_170119/2512947105.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v.init_sims(replace=True)\n",
      "WARNING - 05:02:15: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c86f82",
   "metadata": {
    "id": "a0c86f82"
   },
   "source": [
    "### Subtask 3: Exploring the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e1f70",
   "metadata": {
    "id": "377e1f70"
   },
   "source": [
    "As mentioned in the lecture, word embeddings are suited for similarity and analogy tasks. Let's explore some of that with our dataset:\n",
    "\n",
    "We look for the most similar words to the name of the famous coffee shop where most of the episodes took place, namely `central_perk` and also one of the characters `joey`. If you have followed the exercise correctly until now, you should see that words like `laying` are similar to `central_perk` and the other main characters are also considered similar to `joey`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7074ffa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7074ffa",
    "outputId": "29c66a0c-d48c-4030-a3e0-132de19d0cb2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conan', 0.5344635248184204),\n",
       " ('laying', 0.5073391199111938),\n",
       " ('hippity', 0.48014700412750244),\n",
       " ('recliner', 0.4704996347427368),\n",
       " ('scene', 0.4687216877937317),\n",
       " ('sitting_couch', 0.4459496736526489),\n",
       " ('mitzi', 0.43868589401245117),\n",
       " ('josephine', 0.4304845631122589),\n",
       " ('crossword', 0.42009297013282776),\n",
       " ('enters', 0.40591785311698914)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['central_perk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e66e435",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e66e435",
    "outputId": "71d50093-dc7b-4df8-de8a-7e9b39ca6e09",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chandler', 0.79517662525177),\n",
       " ('ross', 0.6759696006774902),\n",
       " ('rachel', 0.6685718297958374),\n",
       " ('phoebe', 0.6330486536026001),\n",
       " ('monica', 0.6143048405647278),\n",
       " ('right', 0.5854146480560303),\n",
       " ('hey', 0.580683708190918),\n",
       " ('look', 0.5397475361824036),\n",
       " ('apartment', 0.5388873219490051),\n",
       " ('okay', 0.5228437185287476)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###your code###\n",
    "w2v.wv.most_similar('joey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3591274",
   "metadata": {
    "id": "e3591274"
   },
   "source": [
    "Look at the similarity of `green` to `rachel` (her lastname) and `ross`  and `spaceship` (urelated). The first one should have a high and the second a low score. Finally, look at the similarity of `smelly_cat` ( a song from pheobe) and `song` the similarity should be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5de6309",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5de6309",
    "outputId": "03743bc7-2ac3-4585-ce9c-b23ad406a58c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09799353"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###your code###\n",
    "w2v.wv.similarity(\"green\",\"rachel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "322d0605",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "322d0605",
    "outputId": "22d3f9c3-b395-46a1-cb38-9f90c12c19f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15830864"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###your code###\n",
    "w2v.wv.similarity(\"ross\",\"spaceship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a0a1f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7a0a1f5",
    "outputId": "8358a65d-455e-4c56-e4aa-fdcfa49a837f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48799065"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###your code###\n",
    "w2v.wv.similarity(\"smelly_cat\",\"song\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343365d",
   "metadata": {
    "id": "7343365d"
   },
   "source": [
    "We can also ask our model to give us the word that does not belong to a list of words. Let's see from the list of all 5 characters which one is the most dissimilar?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37bff805",
   "metadata": {
    "id": "37bff805"
   },
   "outputs": [],
   "source": [
    "character_names= ['joey', 'rachel', 'phoebe','monica','chandler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9020cd09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9020cd09",
    "outputId": "9ea1ce1a-12db-4000-f885-16a2372c8185",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joey'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.doesnt_match(character_names)### your code###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abdaec",
   "metadata": {
    "id": "28abdaec"
   },
   "source": [
    "Based on the analogies, which word is to `monica` as `man` is to `women`? (print the top 3 words); you should get `chandler`among the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a270c9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a270c9b",
    "outputId": "a672c5b4-38f3-42db-d8aa-d4f8cdbd2475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rachel', 0.5429796576499939),\n",
       " ('chandler', 0.4787571132183075),\n",
       " ('phoebe', 0.4751168489456177),\n",
       " ('mrs_green', 0.43189340829849243),\n",
       " ('ross', 0.4094097912311554),\n",
       " ('left', 0.37180882692337036),\n",
       " ('room', 0.3692134916782379),\n",
       " ('phone', 0.36619970202445984),\n",
       " ('said', 0.36208784580230713),\n",
       " ('oh_god', 0.35728925466537476)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['monica','woman'],negative=['man'])### your code###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97ae83",
   "metadata": {
    "id": "1c97ae83"
   },
   "source": [
    "Finally, lets use [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) to look at the distribution of our embeddings in the vector space for the character `joey`. Follow the instructions and fill in the blank in the `tsneplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0b8930e",
   "metadata": {
    "id": "d0b8930e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faa51d9e",
   "metadata": {
    "id": "faa51d9e"
   },
   "outputs": [],
   "source": [
    "def tsneplot(model, word):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction for the top 10 most similar and dissimilar words\n",
    "    \"\"\"\n",
    "    embs = np.empty((0, 100), dtype='f')# to save all the embeddings\n",
    "    word_labels = [word]\n",
    "    color_list  = ['green']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    embs = np.append(embs, model.wv.__getitem__([word]), axis=0)\n",
    "\n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    all_sims = model.wv.most_similar([word], topn=sys.maxsize)\n",
    "    far_words = list(reversed(all_sims[-10:]))\n",
    "\n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        embs = np.append(embs, wrd_vector, axis=0)\n",
    "\n",
    "    # adds the vector for each of the furthest words to the array\n",
    "    for wrd_score in far_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('red')\n",
    "        embs = np.append(embs, wrd_vector, axis=0)\n",
    "\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = TSNE(n_components=2, random_state=110, perplexity=15).fit_transform(embs)\n",
    "\n",
    "    # sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "\n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "\n",
    "    # basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "\n",
    "    # adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "\n",
    "    plt.xlim(Y[:, 0].min()-1, Y[:, 0].max()+10)\n",
    "    plt.ylim(Y[:, 1].min()-1, Y[:, 1].max()+10)\n",
    "\n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3880bad8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "3880bad8",
    "outputId": "4d0579eb-fc0b-4f6a-bc33-1ef3db1d7b1d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsneplot(w2v, 'joey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2c270",
   "metadata": {
    "id": "1ce2c270"
   },
   "source": [
    "## **Task 2: Questions on the Conceptual Level (non-programming) (Grade (2 + 1 + 1 + 4) = 8)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa409f",
   "metadata": {
    "id": "c3fa409f"
   },
   "source": [
    "Please answer the following questions in the notebook cells using markdown. Be precise and short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec614b0",
   "metadata": {
    "id": "1ec614b0"
   },
   "source": [
    "### Subtask 1: For gradient descent, what advantage has a decaying learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec059fb",
   "metadata": {
    "id": "bec059fb"
   },
   "source": [
    "<<< your answer >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2b619",
   "metadata": {
    "id": "0af2b619"
   },
   "source": [
    "### Subtask 2: Why is it easier to maximize the log likelihood instead of the \"normal\" likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb75406",
   "metadata": {
    "id": "deb75406"
   },
   "source": [
    "<<< your answer >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9828a2f",
   "metadata": {
    "id": "a9828a2f"
   },
   "source": [
    "### Subtask 3: Name one advantage that fastText has over Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7aeb3b",
   "metadata": {
    "id": "6c7aeb3b"
   },
   "source": [
    "<<< your answer >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07634ed8",
   "metadata": {
    "id": "07634ed8"
   },
   "source": [
    "### Subtask 4: Compute the partial derivate of softmax loss for word2vec with\n",
    "\n",
    "---\n",
    "\n",
    "respect to the center word vector.\n",
    "$$ \\frac{\\partial J}{\\partial v_c} =\\frac{\\partial}{\\partial v_c}\\left[ -log \\left( \\frac{exp(u^T_o v_c)}{\\Sigma_{w \\in Vocab} exp(u^T_o v_c)} \\right)\\right] $$\n",
    "use $U$ to denote the matrix of all embeddings and $y$ for a one-hot vector with a 1 for the true outside word $o$, and $\\hat{y}$ for the predicted distribution $P(w|c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba8a44",
   "metadata": {
    "id": "baba8a44"
   },
   "source": [
    "<<< your answer >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364e05b",
   "metadata": {
    "id": "8364e05b"
   },
   "source": [
    "## **Task 3: Auto-Complete Feature (Grade (2 + 6 + 4) = 12)**\n",
    "\n",
    "Let's get even more practical! In this problem set, you will build your own auto-completion system that you see every day while using search engines.\n",
    "\n",
    "[google]: https://www.thedad.com/wp-content/uploads/2018/05/screen-shot-2018-05-12-at-2-01-56-pm.png \"google auto complete\"\n",
    "\n",
    "![google]\n",
    "By the end of this assignment, you will develop a simple prototype of such a system using n-gram language model. At the heart of the system is a language model that assigns the probability to a sequence of words. We take advantage of this probability calculation to predict the next word.\n",
    "\n",
    "The problem set contains 3 main parts:\n",
    "\n",
    "1. Load and preprocess data (tokenize and split into train and test)\n",
    "2. Develop n-gram based language model by estimating the conditional probability of the next word.\n",
    "3. Evaluate the model by computing the perplexity score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5f84e",
   "metadata": {
    "id": "c9d5f84e"
   },
   "source": [
    "### Subtask 1: Load and Preprocess Data\n",
    "We use a subset of English tweets to train our model. Run the cell below to load the data and observe a few lines of it. Notice that tweets are saved in a text file, where the individual tweets are separated by `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48fc8e51",
   "metadata": {
    "id": "48fc8e51",
    "outputId": "86703163-db42-4f7e-dc7d-8bad731b19f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 500 characters of the data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\n',\n",
       " \"When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\n\",\n",
       " \"they've decided its more fun if I don't.\\n\",\n",
       " 'So Tired D; Played Lazer Tag & Ran A LOT D; Ughh Going To Sleep Like In 5 Minutes ;)\\n',\n",
       " 'Words from a complete stranger! Made my birthday even better :)\\n',\n",
       " 'First Cubs game ever! Wrigley field is gorgeous. This is perfect. Go Cubs Go!\\n',\n",
       " 'i no! i get another day off from skool due to the wonderful snow (: and THIS wakes me up...damn thing\\n',\n",
       " \"I'm coo... Jus at work hella tired r u ever in cali\\n\",\n",
       " 'The new sundrop commercial ...hehe love at first sight\\n',\n",
       " 'we need to reconnect THIS WEEK\\n',\n",
       " 'I always wonder how the guys on the auctions shows learned to talk so fast!? all I hear is djsosnekspqnslanskam.\\n',\n",
       " 'Dammnnnnn what a catch\\n',\n",
       " 'such a great picture! The green shirt totally brings out your eyes!\\n',\n",
       " 'Desk put together, room all set up. Oh boy, oh boy\\n',\n",
       " \"I'm doing it!👦\\n\",\n",
       " 'Beauty Brainstorming in the Alchemy office with and Sally Walker!\\n',\n",
       " 'Looking for a new band to blog for the month. anyone interested?\\n',\n",
       " 'Packing for a quick move down the street... If only I had some movers...\\n',\n",
       " 'ford focus hatchback?\\n',\n",
       " 'RT : According to the National Retail Federation $16.3 BILLION was spent on #MothersDay last year!!\\n',\n",
       " '“: \"The tragedy of life is not that it ends so soon, but that we wait so long to begin it.\" - W.M. Lewis”\\n',\n",
       " 'More skating! Come by the check out a movie, eat a great dinner and top it off with great times at the ice rink.\\n',\n",
       " 'watch your mailbox! : )\\n',\n",
       " 'Tommorows the day...\\n',\n",
       " 'Good questions. RT : Your #brand will be judged based on its #website. Is your website a good brand ambassador?...\\n',\n",
       " \"Don't care what others think of you, and you will save yourself a lot of mental energy that instead can be used to push you towards success.\\n\",\n",
       " 'This Ron Artest interview... is it possible to die from laughter?\\n',\n",
       " 'Linda! Just looked at my sched & I have to hustle back to Chula for P.M. meetings, so no time 4 lunch. :( Do u meet every Fri?\\n',\n",
       " 'Bum Squad lets get it! RT : Shout that ninja out for winning\\n',\n",
       " \"I love you, and I'm so proud of you. From sitting on those stairs on The X Factor, to now. You boys are my inspiration.♥ :) xx\\n\",\n",
       " \"Maybe some other time I can't slow down, right across that state line right about now\\n\",\n",
       " 'Exhaust leak! arrrgh\\n',\n",
       " 'i Love Reading your magazine (: it always cheers me up\\n',\n",
       " 'Tables are all sold out for the Mystique Masquerade Ball.\\n',\n",
       " 'Make one up! It might help you feel better. Alternative: just scream nonsense at the dumb machine.\\n',\n",
       " 'ya ik and i never asked him to follow me i only mentioned him once in one of my tweets- i didnt do anything else\\n',\n",
       " 'I will <3\\n',\n",
       " 'Great talking to you guys tonight! Looking forward to your piece next week Jon.\\n',\n",
       " 'Small market baseball. You, know...for the 99%.\\n',\n",
       " 'Sing it!\\n',\n",
       " 'it comes on tonight!! not tomorrow!!\\n',\n",
       " 'maybe? just seems they thought up that idea over sunday brunch and thought it was swell.\\n',\n",
       " 'Thanks for the #FF! Awfully good company to be in.\\n',\n",
       " '\"The longer we live the more we find we are like other persons.\" ~ Oliver Wendell Holmes\\n',\n",
       " 'made my list for top 99 women.\\n',\n",
       " 'nice I watched the whole series, LOVED Julia and her mom Erica was such a badass\\n',\n",
       " 'GOP line on Obama gay marriage stance seems to be that he flip-flopped. Really want to use that with Romney as your presidential candidate?\\n',\n",
       " 'I know, I know. Then you kick yourself when the fight goes lopsided. But if the upset DOES happen, wow. Nothing like it.\\n',\n",
       " 'No stress balls or keg and different crew -- but these guys know how to party\\n',\n",
       " \"William Davis attorney says client will plead insanity. He faces first-degree murder charge in woman's death\\n\",\n",
       " '#OccupyMadison GA: \"I always considered myself a liberal until I saw the liberal machine in action. It was ugly.\"\\n',\n",
       " 'Weird thought: we got to witness the change of a millenia, the next change is 40 generations away...imagine what the differences will be!!\\n',\n",
       " 'got a good one?\\n',\n",
       " 'love chris brown\\n',\n",
       " 'Wit them sexy ass lips\\n',\n",
       " 'its sounds good\\n',\n",
       " 'How (un??)fortunate that those days when every status update was a song lyrics were also the days when I first discovered New Wave...\\n',\n",
       " \"I had a bomb ass day Chillin' with friends.\\n\",\n",
       " \"Working on music for Holiday show on the 23 in Santa Barbara!! OPEN BAR!! Me, Tim from Palin White T's and JR Richards from Dishwalla!!\\n\",\n",
       " 'Hey Nate! Thanks for dropping by yesterday. How was your meal?\\n',\n",
       " '#FF : literary lights who brightened my week.\\n',\n",
       " 'Time to shape up! Water, pavement, weights, and zija are going to be my friends. wedding just 4 months away!\\n',\n",
       " 'Report the many things that are positive about the University of Arkansas athletics program.\\n',\n",
       " 'Athletes/celebrities should have a tool to charge $.99 per RT request. $ to support foundations/charities, fans get involved.\\n',\n",
       " 'Enjoy!! Stay cool!!\\n',\n",
       " '\"...yo chick she so thirst...\" aye!!! I really don\\'t know what else they\\'re saying tho except that lol\\n',\n",
       " 'talks in third person.\\n',\n",
       " '. many things can happen. Only YOU can control your reaction to it. Practice this good\\n',\n",
       " '\"You will allow me to continue to do what I love.\" Big Show\\'s wife?\\n',\n",
       " \"haha very cute! Have u heard from julie?? Who's that on her fb I'm tryin not to laugh\\n\",\n",
       " 'and somedays youre the windshield wiper\\n',\n",
       " \"I'm taking Adam! :-)\\n\",\n",
       " 'lets do this\\n',\n",
       " \"Not much, Ben - how's you?\\n\",\n",
       " 'Sarah trying to explain to my teacher YOLO, lmfao .... >>\\n',\n",
       " '#Inspiring RT : Great minds must be ready not only to take opportunities, but to make them. --Colton\\n',\n",
       " 'med school...wow! I could never do that. Am too OCD to be around blood and sick people for long periods of time ;)\\n',\n",
       " 'Sleeping? Not likely. RT : Sleeping to\\n',\n",
       " 'Time for lunch!\\n',\n",
       " 'Not being dependent on anyone for your happiness >>>\\n',\n",
       " 'This humidity is not sitting well with me at all !! #NeedACInSchool D:\\n',\n",
       " 'Do u know whats creepily??? I Hate Cheesecake. Its Cake with Cheese in front of it!\\n',\n",
       " 'thanks. Got it at walmart!\\n',\n",
       " \"Still haven't seen #Super8? Every 88th check-in wins a free movie ticket. 45,347 check-ins so far!!\\n\",\n",
       " 'RT Congratulations to the for advancing in the #stanleycup Playoffs! They defeated the #Sens in a 4-3 series.\\n',\n",
       " 'Hey hey hey , can i meet chaaaa ?!\\n',\n",
       " 'Ghost Hunters makes me cry :( <3\\n',\n",
       " \"Who's going to the OVW show tomorrow? your team dominoski will be in the house! First brew after the show is on me!\\n\",\n",
       " 'just waking up but #therave last night though>>>>>>\\n',\n",
       " 'at six flags.\\n',\n",
       " 'Thanks Tom.\\n',\n",
       " \"They're not applauding because you don't have a button!\\n\",\n",
       " '« im not mad.. it just sucks that i cant show you off.» THIS!.\\n',\n",
       " \"I'm at little league majors try outs in\\n\",\n",
       " 'Where my meatballs at? and Naomi :)\\n',\n",
       " \"Doing some #cufon testing and implementation in #wordpress. What the FOUC d'y'all think of cufon?\\n\",\n",
       " 'lo mejor! it was cool experiencing work through the eyes of an excited teen\\n',\n",
       " 'Me: the only b word u should call a girl is beautful! Colin: or a butterfly!\\n',\n",
       " \"I just think that the zooming is overused. It needs to be done tastefully. And it's not the point.\\n\",\n",
       " 'If you have Firefox or Google Chrome type in \"let it snow\" into google today.\\n',\n",
       " 'hahahahhahah u just made my day :D\\n',\n",
       " 'I can hear \"High and Dry\" through the walls. Amazing.\\n',\n",
       " 'RT : **IMPORTANT** Our new hashtag is #SNTCK please tweet your #specialneeds resources and writings with us to share\\n',\n",
       " 'Proud to have voted twice for the SAG-AFTRA merger. Have you voted yet?\\n',\n",
       " \"this conference is rockin'\\n\",\n",
       " 'Pretty sure that was a broken leg.\\n',\n",
       " 'Keith was also good a smoking in the dugout and being a class a jerk. I grew up a Mets fan..sorry to admit\\n',\n",
       " \"She's calling you back - is having a moment ;)\\n\",\n",
       " \"Reward yourself for just getting up in the morning. It's a wonderful accomplishment!\\n\",\n",
       " 'Big ups to for leaving voicemails that Google can accurately transcribe.\\n',\n",
       " '#GET BETTER\\n',\n",
       " 'Everyone say a prayer for my brother who is at Wrigley field with his wife and kids. She and her daughters are in Cardinals jerseys.\\n',\n",
       " 'Why you actin like that towards me now\\n',\n",
       " 'Have a great evening/afternoon/whatever it is where you are, folks!\\n',\n",
       " 'experience of a life time!\\n',\n",
       " \"Hey how did you like Dogtooth? I watched like six months ago but don't know anyone else who saw it.\\n\",\n",
       " 'The average gossip and complain. The exceptional create and inspire. #leadership RT\\n',\n",
       " \"#OneThingYouShouldntDo Spam someone to DEATH! it's like stalking them ... Like really?!\\n\",\n",
       " 'me too\\n',\n",
       " 'Buffalo we do appreciate you following Poised Creation on Twitter!! :)\\n',\n",
       " 'I promise you can go ten minutes without hanging on his arm.\\n',\n",
       " 'Snap backs and tattoos\\n',\n",
       " 'Fallout from the original, actually\\n',\n",
       " 'who is your favorite tv mom?\\n',\n",
       " 'Hey Giants fans. How ya feeling tonight. Boy, do you look dumb in that stupid panda hat.\\n',\n",
       " 'out of twitter jail i see.\\n',\n",
       " 'IT PUTS THE WOO IN MY WOO!\\n',\n",
       " \"People can keep talking/Thy can say what thy like/I dont worry cause everythings goin' be alright/No one get n way I feeling 4 (JESUS)\\n\",\n",
       " 'Drawn Dead will be at the Wynn today giving out free shirts!\\n',\n",
       " 'the fact that i added an extra \"d\" on the end of \"just\" amplifies its greatness that much more. Oy vey.\\n',\n",
       " 'Hmm, I got hacked--do not open that link in the DM that was sent from my account. Sorry!! Yikes.\\n',\n",
       " 'dont break my butt\\n',\n",
       " \"you're welcome(:\\n\",\n",
       " \"O.. intrigued by ! We're in negotiations for building our new website...\\n\",\n",
       " 'So what do you do in OKC?\\n',\n",
       " 'the fact that lives on an island makes me wanna shoot myself because i need to see her at least every other day to feel sane.\\n',\n",
       " 'Players were Tebowing before Sat NFL Astroturf Collegiate Bowl. Praying in the end zone before the game and Tebowing a great stite\\n',\n",
       " 'Yes they can :)\\n',\n",
       " 'Hahahahaha, I love it. Thanks for the quick birthday lesson :)\\n',\n",
       " '; Im Almost Up To Youu!\\n',\n",
       " 'Tonight should be fun ;)\\n',\n",
       " 'I spy at Zeitgeist Coffee :)\\n',\n",
       " 'Thanks for the retweet!\\n',\n",
       " \"“: be quiet poo balls.” I'm writing this on my wall\\n\",\n",
       " \"Sarkozy? More like Sark-oh-no-he-didn'tzy, am I right?\\n\",\n",
       " 'going through #RNRCHI bag & wondering if people *actually* pay $60 to do the \"ready to run\" 20 mile training run.\\n',\n",
       " \"That's game we lost I can't hate on the Heat Good Game\\n\",\n",
       " 'The #Marlins wish they had 5000 in attendance tonight...\\n',\n",
       " '#9 for Nelson!\\n',\n",
       " 'Tweetin while I walk. Check my walkin soundtrack!\\n',\n",
       " \"are you visiting again?! 'cause that would be awesome!\\n\",\n",
       " \"no it's the interview he told us about in class member?\\n\",\n",
       " 'Who has the bigger stick?\\n',\n",
       " 'These late night walks are still religion to me. Thank goodness for these.\\n',\n",
       " 'Is that a rhetorical question?\\n',\n",
       " 'I want a new phone case. Preferably a sparkly one.\\n',\n",
       " 'Everybody in the class is laughing at this part and I had no idea what was said :p\\n',\n",
       " 'Bestfriend just got hit on my a 30 something year old\\n',\n",
       " 'thanks for the follow... Your Gorgeous btw! ;)\\n',\n",
       " 'oh boo :( and here I was promoting what I thought it would be to a friend yesterday! Let me know if the cobbler needs help ;)\\n',\n",
       " '\"Collaboration addresses complex problems more effectively because there are more perspectives in the room\"-Gary Mangiofico\\n',\n",
       " 'cinco de mayo fiesta at la carreta is packed!\\n',\n",
       " 'Dallas slapped me on my red as cherries sunburn, and im still crying aftr 10 mins., i hate him.\\n',\n",
       " 'is that kind of place. Like in DC. Ladybugs lurv it.\\n',\n",
       " '#Well Done 2\\n',\n",
       " 'I was born 2 weeks early? Haha\\n',\n",
       " 'cant wait till\\n',\n",
       " 'you are gorgeous, em!\\n',\n",
       " 'That’s what I thought, just wanted to check before I rework it.\\n',\n",
       " 'adele gets off a standing ovation right away?\\n',\n",
       " 'Keep moving, and get out of the way\\n',\n",
       " '“: yeah, l could be the bigger person, or you could just shut the fuck up.”\\n',\n",
       " 'anything intriguing going over there? Any clues on who the browns draft next?\\n',\n",
       " 'Being chauffeured in a brand new Range Rover w/ and Happy Derby day!\\n',\n",
       " 'I lost 5 pounds sweating in sparring class today.\\n',\n",
       " 'Let peace find a toe hold in this tumultuous world.\\n',\n",
       " \"Reminder: we play in Dallas tonight at the den. 10p start. See y'all there.\\n\",\n",
       " 'me, too!\\n',\n",
       " 'And thank everyone of you who has taken the time out of your day to listen to us, your all amazing, much love h2p\\n',\n",
       " \"in MD MIAA-A Boys Lax action, St Mary's Annapolis goes into Loyola-Blakefield's house and leaves with a victory. 11-8\\n\",\n",
       " '#FridayThe13th is the best movie ever....and todays date.\\n',\n",
       " 'The poor needs relief, and they need it now! Somebody help!\\n',\n",
       " \"Okay so apparently it's not new? Sorry for having a life and not listening to music all day long. :)\\n\",\n",
       " 'Heard PM was in #Broncos training facility all day, even had a 2 Hour\\n',\n",
       " 'that is hilarious! We need to do what we can to feel young!!\\n',\n",
       " 'I had the weirdest dreams last night.\\n',\n",
       " \"scrapbook class. Next up: hinged reattachment of enclosures w/ wheat paste and Japanese tissue! It's like I'm at nerd camp.\\n\",\n",
       " 'If your girlfriends parents pay her cell phone bill...\\n',\n",
       " \"thanks... I'll definitely be using some Edmundson\\n\",\n",
       " 'This Little Girl by Cady Groves..... I <3 it! Look it up :)\\n',\n",
       " 'YourKoz Games+Find out who this is-$2.00+She concentrates on the calibre and quality of education for Jordanian children & global education\\n',\n",
       " \"can u please RT or follow? i'm one of your biggest fans :)\\n\",\n",
       " 'The tea party movement filled the gap in republican party on intolerance for overspending.\\n',\n",
       " 'Flores says vocational training is \"his backyard.\" Admiration for police, nurses.\\n',\n",
       " 'In Krav Maga there are no rules, no restrictions\\n',\n",
       " \"Thank you! I'm proud of us all!\\n\",\n",
       " 'I love you brianana\\n',\n",
       " '\"As for me and my house, we will serve the Lord.\" And we will embrace our LGBT brothers and sisters.\\n',\n",
       " 'im following u so u cant welch lol\\n',\n",
       " 'Plan on being a DJ (:\\n',\n",
       " 'I actually saw a police officer on horseback galloping through downtown.\\n',\n",
       " \"8th grade was so great though! & we all said we'd all stay close! We should all get together ASAP.\\n\",\n",
       " \"so? It's not the same.\\n\",\n",
       " \"got that vol. 1 return of the bodysnatchers bumpin. Perfect for the gym! I'm going I'n right now!!\\n\",\n",
       " 'is our new Twitter pg. Follow 4 exciting deals, ATX challenges, & insights on #fashion, epicurean, entertainment & the arts\\n',\n",
       " 'Daddy got a new truck(:\\n',\n",
       " 'Freakin is FLAWLESS..in every way!\\n',\n",
       " \"he's next to go bro\\n\",\n",
       " \"Thanks for finally providing a good night's rest! and I had a great time! Hope your run post beers was good!\\n\",\n",
       " 'I love my momma :)\\n',\n",
       " 'Hello new followers!\\n',\n",
       " 'Not sure! Any recommendations? RT : Hey Chicago! What are you eating at the Taste of Chicago this weekend?\\n',\n",
       " 'Mayoral Candidate Forum on education this Thursday January 21, 6pm at Warren Easton, 3019 Canal street.\\n',\n",
       " 'Ah I miss that place so much! We def need to do another road trip this summer\\n',\n",
       " 'Join the Maui Mall Shoppers Club at www.mauimall.com. We give away three great prizes monthly, including $100 gift certificate to the mall!\\n',\n",
       " 'busy day. I thought called about Film but it was same name dude who is a set medic. #internmistakes!\\n',\n",
       " 'total white cat move\\n',\n",
       " 'I actually liked Derek Morris as a Ranger.\\n',\n",
       " 'Twovsuns massage is now offering couples massage! What a great way to start off your valentines day! Call to schedule today!\\n',\n",
       " \"Stellar of to reply. #digiday I find the lack of women in marketing/pr leadership interesting- it's an area of research for me.\\n\",\n",
       " \"Enter to win an Orioles vintage cap by email or tweet the answer to: Which 2 O's pitchers hold the record for the most opening day starts?\\n\",\n",
       " 'Prepare for testing season by practicing to read standardized instructions in dry monotone.\\n',\n",
       " 'What Is A Life Without A Purpose? What Is A Purpose Without Love?\\n',\n",
       " 'got to fa the one time one time and nope that shit stressede the hell out boa\\n',\n",
       " \"that's the beauty of self-publishing, you can find your audience without publishing types.\\n\",\n",
       " \"i'm waiting patiently for the videos of the tram ride and pics at Yoga! :-)\\n\",\n",
       " 'Congrats to Patti Groff, CLM on winning the Spirit of ALA Award - a huge honor recognizing leadership and dedication!\\n',\n",
       " 'Finally Going to bed. Had a long night but atleast it was for a good cause. ❤#RelayForLife ❤\\n',\n",
       " 'Haha. Works for me sometimes. ;)\\n',\n",
       " 'Send details and I can post.\\n',\n",
       " 'Wearing my 2011 shirt to the #Skrillex show in Indy tonight. Already got a lot of compliments.\\n',\n",
       " 'Howdy Ali - good to meet you too!\\n',\n",
       " 'The scent of Coco Chanel perfume will aways flood me with lovely memories of my dear Aunt Lynn.\\n',\n",
       " '\"We All Got That One Friend whose laugh is always funnier than the joke.\"\\n',\n",
       " 'Fact: Girls have a lot of crushes but their heart only belongs to one guy.\\n',\n",
       " 'Number one SEO mistake? Not doing on page keyword optimization.\\n',\n",
       " 'Count on me! ;) RT : better get your own party started!\\n',\n",
       " 'I hate when i wear bhoops and people are like \" the bigger the hole the bigger the hoe\" and im just like. O___o bitch try me.\\n',\n",
       " 'Be the light in the world of darkness\\n',\n",
       " '#kids Little Stage Puppet Theater: Little Stage Puppet Theater Friday, May 11, 2012 at 6:30 PM Monrovia California 518 E. Colorado,...\\n',\n",
       " \"lost one dream, so now i'm gonna pursue an even greater one\\n\",\n",
       " 'Interested to see matte football helmets next year. Not convinced, yet.\\n',\n",
       " 'oh man I want some French toast so bad now...\\n',\n",
       " 'You mean... a winky face is not the correct use?!\\n',\n",
       " 'omg Jesus loves me SO much that sometimes my hair falls out. Then I pick it up and burn it as an offering. It feels so nice.\\n',\n",
       " 'No you Not!\\n',\n",
       " 'Hi there Sueanne! Liking the newest avatar! Getting to be beach weather, right?!\\n',\n",
       " \"The trip home feels twice as long. Even with good music and junk food. Too bad I don't smoke anymore.\\n\",\n",
       " ': Thanks for ther RT about Administrative Professionals Day.\\n',\n",
       " 'Tryin to stay up for loiter squad\\n',\n",
       " 'looks like its a jquery problem. in this.selectMenuItem, this refers to the document, right?\\n',\n",
       " 'Can I get a refill?\\n',\n",
       " 'Sitting in the doctors office 😖😰💔💔\\n',\n",
       " 'ha nah that aint me\\n',\n",
       " 'Hey everyone, please follow one of the best metal drummers ever, Paul Bostaph,\\n',\n",
       " \"Wow. Bollywood and Smash are still trending. That's how good tonight's episode of #smash was!\\n\",\n",
       " \"seriously? I'll have to check it out!\\n\",\n",
       " 'Just saw this tweet, it was good to meet you too! Thank you so much for your support! MT\\n',\n",
       " 'are you coming to get Donovan\\n',\n",
       " 'OK just asked to have it shipped should have it in the next few days\\n',\n",
       " 'Engineering Dev Manager-excellent leadership opportunity in Bellevue\\n',\n",
       " 'Sometimes \"no syntax error\" doesn\\'t equal finished; it equals actually beginning.\\n',\n",
       " 'Listening to The Faint always makes me think of you :) Its not bad for paper-writing music!\\n',\n",
       " 'Sleep with one eye open.\\n',\n",
       " 'Loved that beer, thank you for sharing\\n',\n",
       " 'Successfully set up Nokia Education Delivery server locally (MacBook Pro) but am at a loss for user guides.\\n',\n",
       " '/ That, Alberto, is part of our \"secret sauce\" for the game. They are not aware, as that could bias gameplay.\\n',\n",
       " 'hell yeah! How much are the tickets?!\\n',\n",
       " 'Thank God, Lifetime, and Thank You John! I hope the show lasts forever on television, this show is very valuable to all of us!\\n',\n",
       " \"Teen Wolf Trailer Omg I can't wait till it comes back on!!(:\\n\",\n",
       " 'U Said \"I Will BE THERE WITH YOU\",But #Wisconsin NEEDS U NOW. GO spend & campaign there\\n',\n",
       " 'Late night last night.\\n',\n",
       " '#Pittsburgh WPXI interviewing family across st whose chimney fell down during the earthquake. Tempted 2walk out dressed in my Batman outfit\\n',\n",
       " 'Time to dream bout some crazy ishhh goodnight 💤\\n',\n",
       " 'okaay (; LOOOOL i still remember our thoughts\\n',\n",
       " \"it's my fault I'm sorry\\n\",\n",
       " \"hey Clifbar people, love your stuff but need more info on this arsenic in organic bar business in the news today. What's your take?\\n\",\n",
       " 'Chicago roadtrip status: ACTIVE.\\n',\n",
       " \"Watched 'Anatomy of a Murder' tonight. Have u seen it? Late-50s courtroom drama that you would surely appreciate.\\n\",\n",
       " 'Happy Mothers Day all Moms from me& FloboRojan RT: , \" Yearning 2 hear well done good & faithful servant..From my Wife.\" (cont)\\n',\n",
       " 'Hard to believe but The Great Online (formerly ReignNet) has accumulated over 150 customers!\\n',\n",
       " 'Griffin shoots on his way down. Irks me.\\n',\n",
       " 'Hey shawty, I was thinkin of you (;\\n',\n",
       " 'theyre gonna win just to fly to miami tomorrow\\n',\n",
       " \"sweet! Let me know what you think. “: can't wait to read Humanize by !”\\n\",\n",
       " 'Stop trying to control everything and just let go.\\n',\n",
       " 'It is an absolutely gorgeous December moon.\\n',\n",
       " \"God has so many amazing plans for you :) I can't wait for whats to come and some new music!!\\n\",\n",
       " \"yup-there's all kinds of sneezing & sniffles in my house lately.\\n\",\n",
       " \"we're apart at a ray\\n\",\n",
       " 'Holy shit my mom can talk. About the SAME. DAMN. THING. That everybody already knows.\\n',\n",
       " 'Coming up on a year with Twitter and not much to show for it...\\n',\n",
       " 'Ok I need a remote starter asap:-(\\n',\n",
       " 'I could go for a nice post dinner cheese plate. What kind of host are you, ?\\n',\n",
       " 'Just had the best snow cone, imperial woodpecker in NYC- get there\\n',\n",
       " 'I need to see so many movies.\\n',\n",
       " 'Why do I always buy the biggest wedding gift?\\n',\n",
       " 'younger guys is in the rotation hopefully behind CC and then RF prob with Swisher and catcher\\n',\n",
       " 'Thanks! Still need Help/Suggestions! I had another 6 commitments Saturday from the young youth group.\\n',\n",
       " \"I deserve to know it's GMO.\\n\",\n",
       " 'Started today at 5 with meditation and a double iced espresso. I think I have found the magic morning combo!\\n',\n",
       " \"I'ts your blog and you are well aware of the link. Don't play dumb with me.\\n\",\n",
       " 'Need to vacuum\\n',\n",
       " 'The panna cotta is a must make!\\n',\n",
       " \"If this whole journalism thing doesn't work out, I think I now know what I want my second career to be.\\n\",\n",
       " 'because they\\'re \"bad bitches\" aka bitches doing bad, close to the lowest female life form, next to jersey chasers\\n',\n",
       " 'Man I am in the weeds today at work. Someone bring me ZKabob and brighten my day :)\\n',\n",
       " \"Don't forget.... our 5th annual #Cuts4aCause is TOMORROW from 9-3!!! Don't miss out on this amazing event!! Call 414.988.4165 to book!\\n\",\n",
       " \"At first I wasn't sure (because I'm vegetarian), but some of their menu does look good; especially the cheeses.\\n\",\n",
       " \"I'll go without before i beg for something...\\n\",\n",
       " 'RT : And IU moves on to the next round of the tournament!\\n',\n",
       " 'Silverton \"Skip the Strip\" New Year\\'s Eve celebration. $10 wristband all you can drink 9:20-12:30\\n',\n",
       " 'I need two new tires .\\n',\n",
       " 'NBA #FanNight Chris webber my dad likes your glasses! DC13\\n',\n",
       " \"sometimes you gotta do stuff you really don't wanna do.\\n\",\n",
       " '- Sounds good bro\\n',\n",
       " \"I'm digging The Offspring right now.\\n\",\n",
       " 'Thanks for the RT! :)\\n',\n",
       " 'Enjoy this sunny day with a smile on your face and your head held high cuz u never know when the dark clouds will roll in\\n',\n",
       " \"because... Why? Doesn't look like a keg. Guess it usually smells like one, I guess.\\n\",\n",
       " 'No worries! We hope you find ours interesting and helpful as well!\\n',\n",
       " \"#Spammers I'd like to see the statistic on what spammers cost american small businesses in a dollar amount. They cost me so much...\\n\",\n",
       " 'Cory Booker is on my tv.\\n',\n",
       " 'If the #2 pencil is so popular then why is it number 2? 😳\\n',\n",
       " 'plz go follow she follow back ;D and i will follow you too :D\\n',\n",
       " \"Chasing The Sun is honestly a very very amazing song. IT'S SO ADDICTING, lol.\\n\",\n",
       " \"Brandon don't joke like that\\n\",\n",
       " 'RT Drake Be Sounding Like He Be Crying His Ass Off When He Be Singing Sometimes ! lOl hell yeah\\n',\n",
       " '\" Motivation and inspiration energize people, not by pushing them in the right direction as control mechanisms do but by satisfying basic...\\n',\n",
       " 'I thought you were strictly Infrastructure bro? You doing sharepoint dev roles too now?\\n',\n",
       " 'no I had the kids. The ex kinda dropped them on me\\n',\n",
       " 'Playing with fire again.\\n',\n",
       " 'Ad2\\'s \"When Creatives and Non-Creatives Unite\" is March 13th. Stay tuned in to Twitter and your Email for more information!\\n',\n",
       " 'awesome! Although a little jealous bout the Jordans! Haha!\\n',\n",
       " 'join the club that plays #unfollowfriday every week! It relieves stress!\\n',\n",
       " 'you said that u cant say boys like my head...yet i have a bf..so obviously he likes my head\\n',\n",
       " 'Do you need a custom \"welcome gate\" for your business page? We can do it! Need social media management? We can handle that for you, too!\\n',\n",
       " '-OR- \"Moons Over my Hammy\" lol M\\n',\n",
       " 'Missing a very important page from my booking notebook...this isnt good.\\n',\n",
       " 'gets a standing ovation on Leno!! Freedom is catching.\\n',\n",
       " \"Sorry for the technical difficulties tonight guys. One day soon we'll have proper working equipment!\\n\",\n",
       " 'it should keep that same letter everytime u plug it in.\\n',\n",
       " \"My Facebook doesn't look any different. Can I say Facebook on Twitter?\\n\",\n",
       " 'The thing I wish I could say to you but knowing I wont<<<\\n',\n",
       " 'is doing some work! Check them out.\\n',\n",
       " 'got the 12th pick,12team league,somebody help me....\\n',\n",
       " 'you never know!\\n',\n",
       " 'That song still gives me chills!\\n',\n",
       " 'Aight heat nation victory nite 2nite... Lets go heat\\n',\n",
       " 'I love talking to this kidd\\n',\n",
       " \"Thin air will help so the offense won't have to drive as far.\\n\",\n",
       " \"Whoa! Whoa! Koala's get chlamydia?\\n\",\n",
       " 'RIP to the day when Titanic sent more than 1,500 people to a watery grave\\n',\n",
       " 'Lakers would do well to keep drawing fouls on the Thunder bigs.\\n',\n",
       " 'is there a worse place to get drafted than Cleveland?\\n',\n",
       " 'can somebody make me a spaghetti diagram showing the connections among all of these data citation programs? Overwhelming...\\n',\n",
       " \"Well, we're getting close to completing the album!..See the journey!\\n\",\n",
       " 'Going to bed, Night <3\\n',\n",
       " 'Just finished \"one thousand gifts\"....what a read. Just ordered 4 new....hate waiting!\\n',\n",
       " \"I just love when my mother forgets to pay my phone bill. Sure hope I don't have a wreck on the way home and have to call 911.\\n\",\n",
       " \"I just swabbed for Janet Liang tonight! And I'm actually home tonight to listen!\\n\",\n",
       " 'Josh Hamilton CRACKS on CJ for 19 home runs on the season!!! Dude is in a ZONE!!!\\n',\n",
       " 'Does anyone get blown out on the road more often than the Lakers?\\n',\n",
       " 'girls over 150 should stick to sweats and bags over their heads\\n',\n",
       " 'Is your last name Trump? Mine got $5 for the first & $1 for each thereafter.\\n',\n",
       " 'haha its the culture B haha\\n',\n",
       " 'LA MUJER DE IVAN @ 8:30PM. Screened as part of The New Chilean Directors Showcase.\\n',\n",
       " \"haha your full of it! Its a gray day and I'm stressed but doing OK. Ha!\\n\",\n",
       " 'Well there might not be any crying in baseball, but there is a great deal of it in skiing.\\n',\n",
       " 'I would love to connect sometime. Where are you located?\\n',\n",
       " 'yessir you are correct\\n',\n",
       " \"I want steak and crab so bad right now. I'm thinking a late night grilling session later.\\n\",\n",
       " 'You will absolutely love it after the first 75 pages. So, suck it up and get that far. It turns on a dime.\\n',\n",
       " \"You know, football isn't nearly as fun to watch when beer isn't involved\\n\",\n",
       " 'Being abused\\n',\n",
       " 'I found Toure Neblett much less of a jerk than one name Toure, the delusional ego maniac with an opinion on everything.\\n',\n",
       " 'Anyone out there looking to buy hunting land in Northern & Central Wisconsin this fall?\\n',\n",
       " \"OMG MediaTemple NameSevers are down! Somebody's going to be in trouble.\\n\",\n",
       " '\"#Mathematics is made of 50 percent formulas, 50 percent proofs, and 50 percent imagination.\" And #mathematicians prove this all the time!\\n',\n",
       " 'can I set up an interview with you guys?\\n',\n",
       " 'wow I have snake bites again\\n',\n",
       " 'Phillip Humber threw a perfect game will never stop talking about it haha\\n',\n",
       " 'Happy Casimir Pulaski Day! Only in Illinois...\\n',\n",
       " 'Guys.. make me upset.\\n',\n",
       " 'Is everyone Getting Along??\\n',\n",
       " \"So it's worth watching? So afraid to commit to another show.\\n\",\n",
       " \"Yes, you should be mean to the foster dog so he doesn't like you. That's the ticket.\\n\",\n",
       " 'aw. Okay\\n',\n",
       " 'I want to get 100 followers already! Follow me!\\n',\n",
       " 'i guess they think they doin somethin by changing my fb password nd im the Lil Kid lmaooo\\n',\n",
       " \"I'd like to know how much household help & had when the boys were infants & in school!\\n\",\n",
       " 'iv never felt this for any guy. u must be special <3 its a thing called true love <3\\n',\n",
       " \"I didn't say it wasn't a good question. I said its not a good question on Mother's Day\\n\",\n",
       " 'Surrendering isn\\'t just about letting go...It\\'s about letting God!\" I surrender all!\\n',\n",
       " '“: Rooney Rule: You should start #eating right, getting #sleep and #exercising soon, because soon it will be too late!”\\n',\n",
       " \"Richard Welsh is Steve's Dad. He is hoping #TeamSoaringHigh from Colorado, where the Welsh family lived a country road or 2 from each other.\\n\",\n",
       " '!++-~| G1 CERTIFIED WET TSHIRT CONTEST --- FRIDAY CLUB DRAMA --WANT TO GET IN FOR FREE?? TXT ME I WILL TELL YOU HOW---214 609 3316 --\\n',\n",
       " '#Startingnow I want ALL those girls of the world to know that they are beautiful and that there should #notbebulling and it should stop!\\n',\n",
       " 'I miss Ryan Dunn =(\\n',\n",
       " 'What a beautiful fall night in Seattle! I love this time of year\\n',\n",
       " \"Now that everyone has graduated, I'm off to never never land\\n\",\n",
       " 'My Name Is MAHATAA is BOOK OF THE MONTH at Agape for the month of February, leading into book signing in mid-March! See you then!\\n',\n",
       " 'good cause the cowboys could use him in two years\\n',\n",
       " 'San diego mayor talking about the necessity of art despite hard times!\\n',\n",
       " 'Some things are easier said than done.\\n',\n",
       " 'Thanks for the RT! I hope all is well with you!!\\n',\n",
       " \"Kind of wish I wasn't going to prom...\\n\",\n",
       " \"Lol. Shep Jr. don't care!\\n\",\n",
       " ': ALL seized Full Tilt #Poker funds, incl. payment processors, should be used to reimburse players. THX\\n',\n",
       " 'Congrats to all who made last night a sellout..come\\n',\n",
       " 'yikes..\"since 2007, RIM has introduced 37 models\" -- A Boggle of Blackberrys nyti.ms/tKK5hZ\\n',\n",
       " 'IDIOT!!! Ban him!!\\n',\n",
       " 'Ok I need kush rehab real shit\\n',\n",
       " 'RT: it would be amazing and the best thing yet... if u were to follow me yee:)) 11:11\\n',\n",
       " 'Taking orders for custom clips, pic sets and webcam shows!!!! ;)\\n',\n",
       " 'Lobby of Marriott playing Bruce (Hungry Heart) waiting for NJ crowd to arrive\\n',\n",
       " \"I didn't get home until the saw the sun coming up... I went hard last night\\n\",\n",
       " 'ugh i know.......long night Smh\\n',\n",
       " 'Sad. The guy who created the Mustang Cobra passed away today. My favorite car!\\n',\n",
       " 'UK sucks. UofL all day\\n',\n",
       " 'One day.... :) “: dude! You guys need to come through Canada already haha”\\n',\n",
       " 'Tommorow is B-day nd Everyone should Wish Her A Good One. She is One Of Are Biggest Supporters. So S/O Nd Love You Pintos :-)\\n',\n",
       " 'Take a risk today! Cooperation is key to executing complex business tasks – but competition is at the heart of innovation.\\n',\n",
       " '#Brewers upcoming series this week: 2 games at NY Mets / 2 games at Houston / 3 games vs Twins\\n',\n",
       " '\"life is too short, gotta live it long\"-Chris Rene, love it!!\\n',\n",
       " \"Started revising the chapter. Your suggestions are helping this argument along so well. I can't thank you enough!\\n\",\n",
       " \"I know that. But others don't.\\n\",\n",
       " \"Have you given your presentation yet? How's the conference going?\\n\",\n",
       " '\"I look all summery and like a volleyball player\" made my day !\\n',\n",
       " \"Yea..I curse like a fucking sailor. But I'm working on it lol\\n\",\n",
       " \"Dream interpretation: It's about sex. That'll be $125.\\n\",\n",
       " 'send the pizza in the mail. :)\\n',\n",
       " 'I guess so. Sure hope its soon enough\\n',\n",
       " 'Up early thiis morning nd now its back to the studio. Got a couple traxxs to listen too nd possibly layiin dwn a track. W.O.R.D.\\n',\n",
       " 'Another Great Workout This A.M. Just Smashed This Breakfast. Now Headed n to the office for a Productive Work Day\\n',\n",
       " \"#ICantStandPeople who think they know it all or start a conversation but don't keep it going\\n\",\n",
       " 'Why do the last words of tweet line 1s repeat at start of line 2s?\\n',\n",
       " 'RT probably a defense mechanism against dissonance from viewing oneself as civilized while acting savage/barbaric toward others.\\n',\n",
       " 'I am so glad you like it as much as I do!\\n',\n",
       " 'I love getting drunk on occasions. Are u out or in home?\\n',\n",
       " 'talking about moving beyond \"the idea of giving your reputation to a social network and letting them monetize it\"\\n',\n",
       " 'Melo is a scorer nobody can stop him at all! He is usually threading scorer not this year though\\n',\n",
       " 'following u\\n',\n",
       " '« I want a large cherry berry chiller .»\\n',\n",
       " 'screw everyone and come to Baylor\\n',\n",
       " 'So happy to finally be home. I missed my family so much.\\n',\n",
       " \"you've been fed a complete diet of propaganda your whole life...now how will you vote?...the only way you have learned thru inculcation?...\\n\",\n",
       " 'I likee both of them ;o\\n',\n",
       " 'TWEET this: #AmpTopTrend with the name of the song or artist you want to hear on . Every hour you choose a new song!\\n',\n",
       " \"i can't even begin to tell you the feeling i get..when hearing my beats play..better than any drug. -McM\\n\",\n",
       " 'is on Tuesday and watching A Man Apart...one of my favorites!\\n',\n",
       " 'Beyond frustrated... Shit!\\n',\n",
       " \"no way you looked like a boy BUT- I feel you sister! Judy cuts my hair and she's a bad ass. She will make you look great.\\n\",\n",
       " \"hey. why'd you decide to join twitter? hopefully i can be your first reply.\\n\",\n",
       " 'Assert your right to make a few mistakes. If people can’t accept your imperfections, that’s their fault.\\n',\n",
       " 'I wuvvvv you\\n',\n",
       " 'I knew something was going to go wrong\\n',\n",
       " 'I will happily provide cookies in exchange for answers. Very sad to hear about #Awake you are an exceptional writer.\\n',\n",
       " \"hahah! It's a heart!\\n\",\n",
       " \"Hahaha! I'm sure he'd indulge you :). For some reason I'd love to also hear Tim Burton's opinion about Inception... :P\\n\",\n",
       " \"She said put a candle on my back baby blow it. I'll blow dat back out aaiight\\n\",\n",
       " \"Romney shouldn't feel too bad about getting whipped today. He did end up winning American Samoa, so he's got that going for him.\\n\",\n",
       " 'These are the days of miracles and wonders. Believe and it shall be\\n',\n",
       " 'Forgiveness comes by growth. Your growth.\\n',\n",
       " \"I used to give a damn, but I'll never give a fuck\\n\",\n",
       " '10 Tablas Creek Vermentino. Delightfully true to type. Sweet tart balanced. Crisp, bit spritzy, green apple, pear, citrus 89 pts\\n',\n",
       " '\"I\\'ll just have the salad\"\\n',\n",
       " 'i dnt have fb sry. but if i did i would ill RT\\n',\n",
       " 'USDA Completely Deregulates Genetically Engineered Alfalfa-WTF!!!!\\n',\n",
       " \"Live music tonight in for a drink or two. Don't forget free parking!\\n\",\n",
       " \"Vacay can't come soon enough.....ugh\\n\",\n",
       " 'Whats up twitter world?!?!?!?!\\n',\n",
       " 'Home from S+J. Gotta get some rest for the 3am Black Friday shopping trip with the man I love . . . Mastercard. I mean, Matt. LOL. *z*\\n',\n",
       " \"#EllenRTme day 159 & 160 It's been such a great weekend & I have no iPhone so this is a 2-for-1!\\n\",\n",
       " 'CONGRATS! Plan on directing or being a DP?\\n',\n",
       " '...and I just drooled coffee all over myself...HAPPY FRIDAY!\\n',\n",
       " 'how many ppl in it ?\\n',\n",
       " 'The retina is the film of the eye. It converts light rays into electrical signals and sends them to the brain through the optic nerve.\\n',\n",
       " 'Sharon Osbourne has morphed into a Cabbage Patch Doll.\\n',\n",
       " 'im really really really obnoxious.\\n',\n",
       " \"Should do a video to that classic shit I'M KILLIN EM\\n\",\n",
       " 'Awwh Love how much you care for us(: it means alot knowing you love us and try to stay in touch with us!!(: we love so much!(:\\n',\n",
       " \"Ha! Which article? And shouldn't the missing period FAILED to stop you in your tracks? Or maybe, omg,the New Yorker is pregnant!\\n\",\n",
       " \"no, it's a marker!\\n\",\n",
       " \"It's Mom's day so I probably won't be around much today. Kiss your mommy for me! xoxoxo\\n\",\n",
       " 'Awww you thought Brown was going to say something\\n',\n",
       " \"I keep taking my temperature on the royal wedding and realizing that I don't care.\\n\",\n",
       " 'Jam lead the D-Fenders 29-19 after one period of play\\n',\n",
       " 'u got a service pack?\\n',\n",
       " 'hey as long as you get to celebrate.\\n',\n",
       " 'Im sending my love out to everyone with intensity purpose & passion! ((hugs))\\n',\n",
       " 'Killed my first NYC #cockroach last night w/ - sign me up for the worlds strongest man competition!\\n',\n",
       " \"hey man. Thought about you last week - watched the 'my star' video. Thanks so much again for that!\\n\",\n",
       " '#LazySundayafterafun saturday night. LOVING/LIVING THE MIAMI LIFE in SHALLAH.\\n',\n",
       " 'Storm headed toward La Plata? Uh oh.\\n',\n",
       " \"Don't worry about it. Won't change in the course of a day\\n\",\n",
       " 'can I breath?\\n',\n",
       " '#FF to for a fantastic #OpenAccess #OA meeting of the minds this week.\\n',\n",
       " \"Hello hello! How are ya today? I'm feeling pretty great myself. I had #SUBWAY for breakfast.\\n\",\n",
       " 'ActorFest 2010!! Visist us @ the Academy booth to enter to Win a free adimssion to a Casting Director Workshop comlpiments of the Academy!\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
    "\n",
    "\n",
    "with open(\"./twitter.txt\", \"r\") as f:\n",
    "    data = f.readlines() #changed the code from read to readlines to make data a list of lines\n",
    "print(\"First 500 characters of the data:\")\n",
    "display(data[0:500])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa007e1d",
   "metadata": {
    "id": "aa007e1d"
   },
   "source": [
    "Now we need to separate the tweets and split them into train and test set. Apply the following pre-processing steps:\n",
    "\n",
    "1. Split data into sentences using \"\\n\" as the delimiter and remove the leading and trailing spaces (drop empty sentences)\n",
    "2. Tokenize the sentences into words using SpaCy and lowercase them. (notice that we do not remove stop words or punctuations.)\n",
    "3. Divide the sentences into 80 percent training and 20 percent test set. No validation set is required. Although in a real-world application it is best to set aside part of the data for hyperparameter tuning.\n",
    "4. To limit the vocabulary and remove potential spelling mistakes, make a vocabulary of the words that appear at least 2 times. The rest of the words will be replaced by the `<unk>` symbol. This is a crucial step since if your model encounters a word that it never saw during training, it won't have an input word that helps determining the next word for suggestion. We use the `<unk>` word for **out of Vocabulary (OOV)** words. Keep in mind that we built the vocabulary on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16d5b640",
   "metadata": {
    "id": "16d5b640"
   },
   "outputs": [],
   "source": [
    "sentences = [datas.strip() for datas in data if datas.strip() ]#split\n",
    "#remove spaces and drop empty sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2faabb7a",
   "metadata": {
    "id": "2faabb7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushal/.config/python.env/nlptrans.env/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = []# list of list of the tokens in a sentence\n",
    "##Your Code###\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    tokensied_sentence = [token.text.lower() for token in doc]\n",
    "    tokenized_corpus.append(tokensied_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "235d1b53",
   "metadata": {
    "id": "235d1b53"
   },
   "outputs": [],
   "source": [
    "from random import Random\n",
    "Random(4).shuffle(tokenized_corpus)\n",
    "\n",
    "split_ratio = 0.8\n",
    "total_sentence = len(tokenized_corpus)\n",
    "split_idx = int(split_ratio*total_sentence)\n",
    "train = tokenized_corpus[:split_idx]##Your Code###\n",
    "test = tokenized_corpus[split_idx:]##Your Code###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2526e4b1",
   "metadata": {
    "id": "2526e4b1",
    "outputId": "dbd43c45-ef6a-417f-97ac-9639346635d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14861\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "flatten_corpus = [word for sentence in train for word in sentence]### Flatten the train corpus ###\n",
    "word_counts = Counter(flatten_corpus)### count the number of each token ###\n",
    "vocab = [word for word,count in word_counts.items() if count>=2]\n",
    "\n",
    "### keep only the ones with frequency bigger than 2 ###\n",
    "print(len(vocab)) ### should be 14861 ###\n",
    "train_replaced = []\n",
    "test_replaced = []\n",
    "for sentence in train:\n",
    "    ### adjust the sentence to contain the word in the vocabulary and <unk> for the rest ####\n",
    "    sentence_replaced = [word if word in vocab else \"<unk>\" for word in sentence]\n",
    "    train_replaced.append(sentence_replaced)\n",
    "\n",
    "for sentence in test:\n",
    "    sentence_replaced = [word if word in vocab else \"<unk>\" for word in sentence]\n",
    "    test_replaced.append(sentence_replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447294a",
   "metadata": {
    "id": "f447294a"
   },
   "source": [
    "### Subtask 2: N-gram Based Language Model:\n",
    "In this section, you will develop an n-grams language model [**1. Large Language Models (LLMs), slide 1-24**]. We assume that the probability of the next word depends only on the previous n-gram or previous n words. We compute this probability by counting the occurrences in the corpus.\n",
    "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ can be estimated as follows:\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})}  $$\n",
    "\n",
    "The numerator is the number of times word '$w_t$' appears after the n-gram, and the denominator is the number of times the n-gram occurs in the corpus, where $C(\\cdots)$ is a count function. Later, we add k-smoothing to avoid errors when any counts are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a590b34",
   "metadata": {
    "id": "7a590b34"
   },
   "source": [
    "To tackle the problem of probability estimation we divide the problem into 3 parts. In the following you will:\n",
    "1. Implement a function that computes the counts of n-grams for an arbitrary number n.\n",
    "2. Estimate the probability of a word given the prior n-words using the n-gram counts.\n",
    "3. Calculate probabilities for all possible words.\n",
    "The steps are detailed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da09885",
   "metadata": {
    "id": "8da09885"
   },
   "source": [
    "Let's start by implementing a function that computes the counts of n-grams for an arbitrary number n.\n",
    "- Prepend necessary starting markers `<s>` to indicate the beginning of the sentence. In the case of a bi-gram model, you need to prepend two start tokens `<s><s>` to be able to predict the first word. \"hello world\"-> \"`<s><s>`hello world\".\n",
    "- Append an end token `<e>` so that the model can predict when to finish a sentence.\n",
    "- Create a dictionary to store all the n_gram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8a98cee",
   "metadata": {
    "id": "e8a98cee"
   },
   "outputs": [],
   "source": [
    "def n_grams_counts(corpus, n):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the corpus given the parameter n\n",
    "\n",
    "    data: List of lists of words (your tokenized corpus)\n",
    "    n: n in the n-gram\n",
    "\n",
    "    Returns: A dictionary that maps a tuple of n words to its frequency\n",
    "    \"\"\"\n",
    "    start_token='<s>'\n",
    "    end_token = '<e>'\n",
    "    n_grams = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "\n",
    "        if n==1:\n",
    "            sentence = [start_token] + sentence + [end_token] ### add start and end token ###\n",
    "        else:\n",
    "            sentence = [start_token]*(n-1) + sentence + [end_token]\n",
    "        # convert list to tuple so it can be used a the key in the dictionary\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_gram = tuple(sentence[i:i + n])\n",
    "            n_grams[n_gram] += 1\n",
    "\n",
    "\n",
    "        ###iterate over the n-grams in the sentence, you can use the range() function, and increament the counts in the\n",
    "        ## n_grams dictionary, where the key is the n_gram and the value is count\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75f530",
   "metadata": {
    "id": "ba75f530"
   },
   "source": [
    "The next step is to estimate the probability of a word given the prior n words using the n-gram counts, based on the formula given at the beginning of this task. To deal with the problem of zero division we add k-smoothing. K-smoothing adds a positive constant $k$ to each numerator and $k \\times |vocabulary size|$ in the denominator. Below we will define a function that takes in a dictionary `n_gram_cnt`, where the key is the n-gram, and the value is the count of that n-gram, plus a dictionary for `plus_current_gram_cnt`, which you'll use to find the count for the previous n-gram plus the current word. Notice that these dictionaries are computed using the previous function `n_grams_counts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2c6898d",
   "metadata": {
    "id": "a2c6898d"
   },
   "outputs": [],
   "source": [
    "def probability(word, prev_n_gram,\n",
    "                         n_gram_cnts, plus_current_gram_cnts, vocab_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of the next word using the n-gram counts with k-smoothing\n",
    "    word: next word\n",
    "    prev_n_gram: previous n gram\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab_size: number of words in the vocabulary\n",
    "\n",
    "    Returns: A probability\n",
    "    \"\"\"\n",
    "\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "\n",
    "    prev_n_gram_cnt =  n_gram_cnts.get(prev_n_gram,0)# get the previous n-gram count from the dictionary\n",
    "    denominator = prev_n_gram_cnt + (k * vocab_size) # denominator with the previous n-gram count and k-smoothing\n",
    "    plus_current_gram =  prev_n_gram + (word,)# add the current word to the n-gram\n",
    "    plus_current_gram_cnts = plus_current_gram_cnts.get(plus_current_gram, 0) # get the current n-gram count using the dictionary\n",
    "    numerator = plus_current_gram_cnts + k #calculate the numerator with k-smoothing\n",
    "    prob = numerator/denominator\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803eb7c",
   "metadata": {
    "id": "7803eb7c"
   },
   "source": [
    "Let's use the functions we have defined to calculate probabilities for all possible words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7157c4cb",
   "metadata": {
    "id": "7157c4cb"
   },
   "outputs": [],
   "source": [
    "def probabilities(prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities for all the words in the vocabulary given the previous n-gram\n",
    "    prev_n_gram: previous n-gram\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cntsplus_current_gram_cnt: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab: List of words\n",
    "\n",
    "    Returns: A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "\n",
    "    vocab.extend(['<e>', '<unk>']) # add <e> <unk> to the vocabulary\n",
    "    vocabulary_size = len(vocab)#compute the size\n",
    "\n",
    "    probabilities = {}\n",
    "    for word in vocab:\n",
    "        ### compute the probability\n",
    "        prob = probability(word, prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocabulary_size)\n",
    "        probabilities[word] = prob\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88fea049",
   "metadata": {
    "id": "88fea049",
    "outputId": "20409995-f5d9-495a-fddb-1e1fa025c67e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'shining': 0.05263157894736842,\n",
       " 'plants': 0.05263157894736842,\n",
       " 'mars': 0.05263157894736842,\n",
       " 'moon': 0.21052631578947367,\n",
       " 'a': 0.05263157894736842,\n",
       " 'tonight': 0.05263157894736842,\n",
       " 'plant': 0.05263157894736842,\n",
       " 'are': 0.05263157894736842,\n",
       " 'is': 0.05263157894736842,\n",
       " 'shinnig': 0.05263157894736842,\n",
       " 'stars': 0.05263157894736842,\n",
       " 'and': 0.05263157894736842,\n",
       " 'bright': 0.05263157894736842,\n",
       " 'the': 0.05263157894736842,\n",
       " '<e>': 0.05263157894736842,\n",
       " '<unk>': 0.05263157894736842}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the probability of all possible words after the unigram \"the\"\n",
    "sentences = [['the', 'moon', 'and', 'stars', 'are','shining','bright'],\n",
    "             ['the', 'moon', 'is', 'shinnig','tonight'],\n",
    "             ['mars','and' ,'moon', 'are', 'plants'],\n",
    "             ['the' ,'moon', 'is','a', 'plant']]\n",
    "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]+ sentences[3]))\n",
    "unigram_counts = n_grams_counts(sentences, 1)\n",
    "bigram_counts = n_grams_counts(sentences, 2)\n",
    "print(\"The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\")\n",
    "probabilities([\"the\"], unigram_counts, bigram_counts, unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a385f",
   "metadata": {
    "id": "014a385f"
   },
   "source": [
    "### Subtask 3: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4345f",
   "metadata": {
    "id": "90a4345f"
   },
   "source": [
    "In this part, we use the perplexity score to evaluate your model on the test set. The perplexity score of the test set on an n-gram model is defined as follows:\n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } $$\n",
    "- where $N$ is the length of the sentence. ($N-1$ is used because in the code we start from the index 0).\n",
    "- $n$ is the number of words in the n-gram.\n",
    "- $W$ is the n-gram\n",
    "\n",
    "Notice that we have already computed this probability.\n",
    "\n",
    "The higher the probabilities are, the lower the perplexity will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9644c33f",
   "metadata": {
    "id": "9644c33f"
   },
   "outputs": [],
   "source": [
    "def perplexity(sentence, n_gram_cnts, plus_current_gram_cnts, vocab_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    sentence: List of strings\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab_size: number of unique words in the vocabulary\n",
    "    k: positive smoothing constant\n",
    "\n",
    "    Returns: Perplexity score for a single sentence\n",
    "    \"\"\"\n",
    "\n",
    "    n =  len(list(n_gram_cnts.keys())[0]) # get the number 'n' in  n-gram  from n_gram_cnts\n",
    "\n",
    "    sentence = ['<s>'] + sentence + ['<e>'] # prepend <s> and append <e>\n",
    "    sentence = tuple(sentence)\n",
    "    N = len(sentence)# length of sentence\n",
    "\n",
    "\n",
    "    product_pi = 1.0\n",
    "\n",
    "    ### Compute the product of probabilities ###\n",
    "\n",
    "    for t in range(n, N):\n",
    "        n_gram = sentence[t - n:t] # get the n-gram before the predicted word (n-gram before t )\n",
    "        word = sentence[t]  # get the word to be predicted (position t)\n",
    "        prob = probability(word, n_gram, n_gram_cnts, plus_current_gram_cnts, vocab_size, k)\n",
    "\n",
    "        product_pi *= prob  # Update the product of the probabilities\n",
    "\n",
    "    perplexity = product_pi ** (1 / float(N))  # Take the Nth root of the product\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745e1bc",
   "metadata": {
    "id": "8745e1bc"
   },
   "source": [
    "Use `perplexity` function to find the perplexity of a bi-gram model on the first training sample and on the first test sample (first element of the set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d857043a",
   "metadata": {
    "id": "d857043a",
    "outputId": "d4c17025-0137-40bb-bb4a-8bd130a42511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 0.0107\n",
      "Perplexity for test sample: 0.0005\n"
     ]
    }
   ],
   "source": [
    "bigram_counts = n_grams_counts(tokenized_corpus,2)### your code ###\n",
    "trigram_counts = n_grams_counts(tokenized_corpus,3)### your code ###\n",
    "\n",
    "perplexity_train = perplexity(train_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
    "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
    "\n",
    "perplexity_test = perplexity(test_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")# the perplexity for the train sample should be much lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c697faa",
   "metadata": {
    "id": "7c697faa"
   },
   "source": [
    "Finally, let's use the model we created to generate an auto-complete system that makes suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6d6163b",
   "metadata": {
    "id": "b6d6163b"
   },
   "outputs": [],
   "source": [
    "def suggest_a_word(up_to_here, n_gram_cnts, plus_current_gram_cnts, vocab , start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    up_to_here: the sentence so far, must have length > n\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab: List of words\n",
    "    start_with: If not None, specifies the first few letters of the next word\n",
    "\n",
    "    Returns: (most likely next word,  probability)\n",
    "    \"\"\"\n",
    "    n = len(list(n_gram_cnts.keys())[0]) # get the number 'n' in  n-gram  from n_gram_cnts\n",
    "    previous_n_gram = up_to_here[-n:] # get the last 'n' words as the previous n-gram from the input sentence\n",
    "\n",
    "    if start_with is not None:\n",
    "        filter_vocab = [word for word in vocab if word.startswith(start_with)]\n",
    "    else:\n",
    "        filter_vocab = vocab\n",
    "    best_word = None\n",
    "    best_prob = 0.0\n",
    "    probabs =  probabilities(previous_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab)    #stimate the probabilities for each word in the vocabulary\n",
    "\n",
    "    ### sort the probability for higher to lower and return the highest probability word,probability tuple\n",
    "    #if start_with is specified then return the highest probability word that starts with that specific character\n",
    "\n",
    "    for word in filter_vocab:\n",
    "        probability = probabs.get(word, 0.0)\n",
    "        if probability > best_prob:\n",
    "            best_word = word\n",
    "            best_prob = probability\n",
    "\n",
    "    return (best_word, best_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f9abc",
   "metadata": {
    "id": "f88f9abc"
   },
   "source": [
    "Test your model based on the bi-gram model created on the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "460dff09",
   "metadata": {
    "id": "460dff09",
    "outputId": "4185d57a-83bf-447c-a6cc-d4b616bd68c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('better', 0.00013286388095396266)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_tokens=['i','like']\n",
    "start_with='b'\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e68f730c",
   "metadata": {
    "id": "e68f730c",
    "outputId": "b0c53113-9c41-4374-c9fd-fa5023805264"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('see', 0.0010005336179295624)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_tokens=['i','like','to']\n",
    "start_with=None\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "001a7ff4",
   "metadata": {
    "id": "001a7ff4",
    "outputId": "9715d385-d10f-426a-ff5e-891791ddb915"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"', 0.000201450443190975)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_tokens=[\"hello\", \"my\", \"name\", \"is\"]\n",
    "start_with=None\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c556e16f",
   "metadata": {
    "id": "c556e16f",
    "outputId": "329e0d83-5046-43b7-db07-b9a12bb2475a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('allison', 0.00013430029546065002)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_tokens=[\"hello\", \"my\", \"name\", \"is\"]\n",
    "start_with='a'\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a15ac8",
   "metadata": {
    "id": "b4a15ac8"
   },
   "source": [
    "## **Task 4: Understanding GloVe (Grade (2 + 4.25 + 4 + 1.75) = 12)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e586ca",
   "metadata": {
    "id": "15e586ca"
   },
   "source": [
    "In this part, you will implement the [GloVe](https://nlp.stanford.edu/projects/glove/) model and train your own word vectors with gradient\n",
    "descent and numpy. GloVe stands for Global Vectors for word representation, which was developed by researchers at Stanford University to generate word embeddings from corpus statistics.\n",
    "The statistics of the corpus are represented by a co-occurrence matrix, indicating how often a particular word pair occurs together.\n",
    "GloVe is based on ratios of probabilities from this co-occurrence matrix, combining the intuitions of count-based models while also being similar to neural models like word2vec.\n",
    "From this matrix, one can compute the co-occurrence probabilities. We motivate this with an example:\n",
    "\n",
    "$P_{ik} = P(i,j)$ -> co-occurrence probability or joint probability of words $i$ and $k$\n",
    "\n",
    "$P_{jk}$ ->co-occurrence probability of words $j$ and $k$\n",
    "\n",
    "$\\frac{P_{ik}}{P_{jk}}$-> corelation between $i$ and $j$ with the prob word $k$\n",
    "\n",
    "This ratio gives us some insight into the co-relation of the probe word $k$ with the words $i$ and $j$.\n",
    "An example is shown below for different prob words $k$, where $i$ and $j$ are `ice` and `steam`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076234c5",
   "metadata": {
    "id": "076234c5"
   },
   "source": [
    "\n",
    "![glove](https://nlp.stanford.edu/projects/glove/images/table.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b933587",
   "metadata": {
    "id": "3b933587"
   },
   "source": [
    "Image taken from Stanford NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8f6f0",
   "metadata": {
    "id": "76e8f6f0"
   },
   "source": [
    "Compared to the raw probabilities, the ratio of probabilities is better able to distinguish relevant words from irrelevant words. Consider the raw probabilities (the first two rows), the values are close to one another and not indicative of the relationships. However, the ratios have more distinct values.\n",
    "\n",
    "This ratio can be small, large, or equal to 1 depending on the prob word and their co-relation. In the example above, the ratio between ice and steam for `k=solid` is large and for `k=gas` is small, indicating that `solid` is related to ice but `gas` is irrelevant. On the other hand, `water` is not a discriminating element between them, and therefore the ratio is close to one. The same applies to an irrelevant word like `fashion`.\n",
    "\n",
    "The GloVe model is built on the idea that the \"ratio of conditional probabilities represents the word meanings\" and a neural model is trained to estimate this conditional probability.\n",
    "\n",
    "$F(w_i,w_j,\\tilde{w_k})=\\frac{P_ik}{P_jk}$ -> the right-hand side is computed from the corpus statistics, $w$ is the word vector and $\\tilde{w_k}$ is the context vector\n",
    "\n",
    "The GloVe model embeds the words in a vector space and claims that the difference between them (distinguishing factor) is hidden in the ratio of probabilities. In vector space, the best way to encode this is by vector differences.\n",
    "\n",
    "$F((w_i-w_j),\\tilde{w_k})=\\frac{P_ik}{P_jk}$\n",
    "\n",
    "At this point, the left-hand side is a vector and the right-hand side is a scalar showing the similarity of i and j with the context word k.\n",
    "For both sides to match and to encode the similarity in the vector space, the left-hand side becomes a dot product.\n",
    "\n",
    "$F((w_i-w_j)^{T}.\\tilde{w_k})=\\frac{P_ik}{P_jk}=\\frac{F(w_i.\\tilde{w_k})}{F(w_j.\\tilde{w_k})} $\n",
    "\n",
    "$F(w_i.\\tilde{w_k})=\\frac{X_{ik}}{X_{i}}$ -> where $X$ is drived from the co-occurrences matrix\n",
    "\n",
    "To satisfy a symmetrical relationship (a.k.a. relation(a, b) = relation(b, a)), $F$ is chosen to be an exponential function, $F(x)=exp(x)$. As a result:\n",
    "\n",
    "$w_i.\\tilde{w_k} = log(P_{ik} ) = log(X_{ik} ) − log(X_{i})$\n",
    "\n",
    "$log(X_{i})$ is independent of $k$ and can be absored into a bias term.\n",
    "\n",
    "$w_i.\\tilde{w_k} = log(P_{ik} ) + b_i +\\tilde{b_k} = log(X_{ik} )$\n",
    "\n",
    "After some weighting and alterations the final cost function, based on the weighted least squares regression model is as follows:\n",
    "\n",
    "$J= \\Sigma^{V}_{i,j=1} f(X_{ij})(w_i.w_j+ b_i +b_j)-log(X_{ik})$\n",
    "\n",
    "For a detailed overview refer to the original paper: https://nlp.stanford.edu/pubs/glove.pdf\n",
    "You need to read and understand the GloVe model to solve this exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d71fbc8",
   "metadata": {
    "id": "0d71fbc8"
   },
   "source": [
    "### Subtask 1: Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174acc1f",
   "metadata": {
    "id": "174acc1f"
   },
   "source": [
    "Read the paper and describe the following, in your own words:\n",
    "\n",
    " 1. The intuition behind the weighting schema in the cost function.\n",
    "\n",
    " 2. How does the objective function of GloVe relate to the objective function of the (word2vec) skip-gram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac9d93",
   "metadata": {
    "id": "9dac9d93"
   },
   "source": [
    "1. The cost function is weighting schema of this function to give more weight to rare word pairs in the co-occurrence matrix. To acount for rare words in the training of word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3887c",
   "metadata": {
    "id": "d1e3887c"
   },
   "source": [
    "2. The objective function of GloVe and the objective function of the skip-gram model relate as both are used to learn word vectors that represents semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ae19d",
   "metadata": {
    "id": "988ae19d"
   },
   "source": [
    "### Subtask 2: Build Co-occurence matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca833e",
   "metadata": {
    "id": "edca833e"
   },
   "source": [
    "We use the same dataset as the first task and use the `quotes` as the corpus to build the co-occurrences matrix. Similar to the first task we use the `phrases` to transform our input and extract the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bca0b788",
   "metadata": {
    "id": "bca0b788"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from math import log\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc60e8",
   "metadata": {
    "id": "dacc60e8"
   },
   "source": [
    "Complete the function to create dictionaries used for mapping ids to words and words to ids, and a dictionary that counts the number of occurrences of each word. The first two dictionaries are used to map indices in vector space to words and back, and the third dictionary contains the counts of the corpus statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09dc3179",
   "metadata": {
    "id": "09dc3179"
   },
   "outputs": [],
   "source": [
    "def create_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Build a vocabulary containing the frequencies\n",
    "    corpus: the list of tokenized lines form the corpus\n",
    "    \n",
    "    Returns  dictionaries `word` -> (index or unique identified), frequency)`\n",
    "    and `word` -> (index or unique identified)\n",
    "    and index or unique identified -> `word`\n",
    "    and length of the vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    word_count_dict = {} # word id to the number of time it appears\n",
    "    id_to_word={} # mapping ids to words\n",
    "    word_to_id={} # mapping words to ids\n",
    "    \n",
    "    # Iterate over each line in the corpus\n",
    "    for line in corpus:\n",
    "        # Iterate over each word in the line\n",
    "        for word in line:\n",
    "            if word not in word_to_id:\n",
    "                unique_id = len(word_to_id)\n",
    "                word_to_id[word] = unique_id\n",
    "                id_to_word[unique_id] = word\n",
    "                word_count_dict[unique_id] = 1\n",
    "            else:\n",
    "                unique_id = word_to_id[word]\n",
    "                word_count_dict[unique_id] += 1\n",
    "                \n",
    "    return word_count_dict, id_to_word, word_to_id, len(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fb4ca54",
   "metadata": {
    "id": "9fb4ca54"
   },
   "outputs": [],
   "source": [
    "word_count_dict, id_to_word,word_to_id, vocab_size=create_vocab(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbc595-356c-4928-a345-241076baca3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "GlX4ugx88_v7",
   "metadata": {
    "id": "GlX4ugx88_v7"
   },
   "source": [
    "If you have done the exercise correctly, you have `15333` tokens in the vocabulary, and the number of occurrences for `joey` is `1951` and for `central_perk` is `36`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8b31695",
   "metadata": {
    "id": "c8b31695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of occurrences for joey: 2283\n",
      "number of occurrences for central perk: 36\n",
      "vocab size is: 15818\n"
     ]
    }
   ],
   "source": [
    "print(\"number of occurrences for joey:\",word_count_dict[word_to_id['joey']])\n",
    "print(\"number of occurrences for central perk:\",word_count_dict[word_to_id['central_perk']])\n",
    "print(\"vocab size is:\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b027280a",
   "metadata": {
    "id": "b027280a"
   },
   "outputs": [],
   "source": [
    "def calculate_weight(cooccurrences, context_word_ids, center_word_id, side):\n",
    "    \"\"\"\n",
    "    Calculate the weight in the co-occurrence matrix based on the distance of a word\n",
    "    to the center word\n",
    "    sentence = [I, went, to , the, bank]\n",
    "    Let center word be \"to\" and window size =2\n",
    "    left_context =[I,went]\n",
    "    right_context = [the,bank]\n",
    "\n",
    "    Weights:\n",
    "    1/distance-> `went` and `the` have weight of 1 and `I` and `bank` have weight of 1/2\n",
    "    \"\"\"\n",
    "\n",
    "    if side == \"right_context\":\n",
    "        context_word_ids.reverse()\n",
    "\n",
    "        ## adjust the weight of the matrix to 1/distance between the center word and context word, where center word will act as the row and the context word is the column##\n",
    "        for index, context_word_id in enumerate(context_word_ids, start=1):\n",
    "            # normalize weights and add to cooccurrences\n",
    "            normalized_weight = 1 / index # The distance of a word from the center word is defined by its position in context_word_ids.\n",
    "            cooccurrences[center_word_id, context_word_id] += normalized_weight\n",
    "\n",
    "    return cooccurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MUF8zXO59P3b",
   "metadata": {
    "id": "MUF8zXO59P3b"
   },
   "source": [
    "The weight of first to second element on the example matrix should be `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7c32c0e",
   "metadata": {
    "id": "d7c32c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight of id=0 to id=1 : 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "cooccurrences = sparse.lil_matrix((10, 10),dtype=np.float64)\n",
    "calculate_weight(cooccurrences, [1,3,5], 0, side=\"right_context\")\n",
    "print(\"weight of id=0 to id=1 :\",cooccurrences[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede37e8",
   "metadata": {
    "id": "0ede37e8"
   },
   "source": [
    "We build co-occurrence as a sparse matrix to speed up the computation. The original matrix is a square matrix in the size of the vocabulary. However, many words do not co-occur with one another and we do not need to store those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d63f39e7",
   "metadata": {
    "id": "d63f39e7"
   },
   "outputs": [],
   "source": [
    "def build_cooccur(corpus, window_size=3, min_count=5):\n",
    "    \"\"\"\n",
    "    Create a coocurrance matrix given a corpus\n",
    "    corpus: the list of tokenized lines form the corpus\n",
    "    window_size: how many words to right and left to consider\n",
    "\n",
    "    Returns the co-oocurrance sparse matrix\n",
    "    \"\"\"\n",
    "    vocab, id_to_word,word_to_id, vocab_size= create_vocab(corpus)\n",
    "\n",
    "    # sparse lil_matrix is optimized to operate on matrix which mostly has zeros.\n",
    "    cooccurrences = sparse.lil_matrix((vocab_size, vocab_size), dtype=np.float64)\n",
    "\n",
    "    for i, line in enumerate(corpus):\n",
    "\n",
    "        # Get the ID of words from vocab dictionary\n",
    "        word_ids = [word_to_id[word] for word in line]\n",
    "\n",
    "        for i, center_word_id in enumerate(word_ids):\n",
    "\n",
    "            left_context_word_ids = word_ids[max(0, i - window_size):i]\n",
    "            right_context_word_ids = word_ids[i + 1:i + window_size + 1]\n",
    "\n",
    "            # update the matrix based on the distance weights on both sides\n",
    "            for left_id in left_context_word_ids:\n",
    "                cooccurrences[center_word_id, left_id] += 1 / (i + 1 - left_context_word_ids.index(left_id))\n",
    "            for right_id in right_context_word_ids:\n",
    "                cooccurrences[center_word_id, right_id] += 1 / (right_context_word_ids.index(right_id) + 1)\n",
    "\n",
    "    # go into the LiL-matrix to quickly iterate through all nonzero cells and filter out the ones with minimum count\n",
    "    cooccurrences_tuples = []\n",
    "    for i, (row, data) in enumerate(zip(cooccurrences.rows, cooccurrences.data)):\n",
    "        cooccurrences_tuples.extend([(i, j, cooccurrences[i, j]) for j in row if cooccurrences[i, j] >= min_count])\n",
    "        \n",
    "    return cooccurrences_tuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aba5bd",
   "metadata": {
    "id": "f3aba5bd"
   },
   "source": [
    "Build a matrix with a window_size of 3 words, and the minimum number of times a word has to occur to be part of the matrix is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69c198e0",
   "metadata": {
    "id": "69c198e0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 622, 14.549639249639249)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix=build_cooccur(new_lines, window_size=3, min_count=10)\n",
    "matrix[103]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc1256",
   "metadata": {
    "id": "82cc1256"
   },
   "source": [
    "### Subtask 3: Modelling and Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91764831",
   "metadata": {
    "id": "91764831"
   },
   "source": [
    "We initialize the weights for the context and center words and learn the vectors through backprop, using the GloVe cost function.\n",
    "Make sure you use the correct weighting schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05251525",
   "metadata": {
    "id": "05251525"
   },
   "outputs": [],
   "source": [
    "# Random normal weights intialization\n",
    "np.random.seed(77)# we set a seed to have similar results\n",
    "def init_weights(vocab_size, hidden):\n",
    "     #Each word has a center word vector and a context vector.\n",
    "    W_center = np.random.randn(vocab_size, hidden)\n",
    "    b_center = np.random.randn(vocab_size, 1)\n",
    "    W_context = np.random.randn(vocab_size, hidden)\n",
    "    b_context = np.random.randn(vocab_size, 1)\n",
    "    return W_center, b_center, W_context, b_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y-mcYRis9wet",
   "metadata": {
    "id": "y-mcYRis9wet"
   },
   "source": [
    "keep track of `W_center[0,1]` as it should change based on backprop in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2f31560",
   "metadata": {
    "id": "b2f31560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the shapes to make sure the matrices have correct sizes:\n",
      "(100, 32)\n",
      "(100, 1)\n",
      "(100, 32)\n",
      "(100, 1)\n",
      "Look at the value of this element and how it changes with back prob:\n",
      "0.6615314728168009\n"
     ]
    }
   ],
   "source": [
    "W_center, b_center, W_context, b_context=init_weights(100, 32)\n",
    "print(\"check the shapes to make sure the matrices have correct sizes:\")\n",
    "print(W_center.shape)\n",
    "print(b_center.shape)\n",
    "print(W_context.shape)\n",
    "print(b_context.shape)\n",
    "print(\"Look at the value of this element and how it changes with back prob:\")\n",
    "print(W_center[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gae189dl-ZZo",
   "metadata": {
    "id": "gae189dl-ZZo"
   },
   "source": [
    "Write a training script for the GloVe model that goes over the entire co-occurrence matrix given a number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8669b4fb",
   "metadata": {
    "id": "8669b4fb"
   },
   "outputs": [],
   "source": [
    "# Back Propagation\n",
    "def back_prop(W_center, b_center, W_context, b_context, matrix, x_max,  vocab_size, learning_rate,alpha=2):\n",
    "    \"\"\"\n",
    "    W_center, b_center: weight and bias of the center word\n",
    "    W_context, b_context: weight and bias of the context word\n",
    "    vocab_size: vocabulary size\n",
    "    x_max: define our weighting function when computing the cost for two word pairs; see the GloVe paper for more\n",
    "    details.\n",
    "    matrix: coocurrance matrix\n",
    "    alpha: the power of x_max function\n",
    "    learning_rate: learning rate for gradient descent\n",
    "    \"\"\"\n",
    "\n",
    "    global_cost = 0\n",
    "    hidden_dim = W_center.shape[1]\n",
    "    for i, j, cooccurrence in matrix:\n",
    "        weight = ((cooccurrence/x_max)**alpha if(cooccurrence < x_max) else 1)\n",
    "        # Compute inner component of cost function  J' = w_i^Tw_j + b_i + b_j - log(X_{ij})\n",
    "        cost_inner = np.dot(W_center[i].T,W_context[j])+ b_center[i] + b_context[j]- np.log1p(cooccurrence)\n",
    "\n",
    "        # Compute cost J = f(X_{ij}) (J')^2\n",
    "        cost = weight * (cost_inner ** 2)\n",
    "\n",
    "        # Compute gradients for word vectors\n",
    "        grad_center_context = weight * cost_inner\n",
    "        grad_W_center = grad_center_context * W_context[j].reshape(hidden_dim, 1)\n",
    "        grad_W_context = grad_center_context * W_center[i].reshape(hidden_dim, 1)\n",
    "\n",
    "        # Compute gradients for bias terms\n",
    "        grad_b_center = grad_center_context\n",
    "        grad_b_context = grad_center_context\n",
    "\n",
    "        # Update the weights and biases\n",
    "        W_center[i] -= learning_rate * grad_W_center.squeeze()\n",
    "        W_context[j] -= learning_rate * grad_W_context.squeeze()\n",
    "\n",
    "        b_center[i] -= learning_rate * grad_b_center\n",
    "        b_context[j] -= learning_rate * grad_b_context\n",
    "\n",
    "        # Accumulate cost\n",
    "        global_cost += 0.5 * cost \n",
    "\n",
    "    return W_center, b_center, W_context, b_context, global_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sV9uZIgX95tZ",
   "metadata": {
    "id": "sV9uZIgX95tZ"
   },
   "source": [
    "Based on the random seed, the value of `W_center[0,1]` should have changed due to backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84a467a0",
   "metadata": {
    "id": "84a467a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: [0.08139524]\n",
      "changed value:\n",
      "0.6618654780189167\n"
     ]
    }
   ],
   "source": [
    "test_matrix=[(0,1,1),(0,2,0.4),(0,3,0.9),(0,4,0.4)]\n",
    "W_center, b_center, W_context, b_context, global_cost  =back_prop(W_center, b_center, W_context, b_context, test_matrix, x_max=10,  vocab_size=100, learning_rate=0.01)\n",
    "print(\"cost:\",global_cost)\n",
    "print(\"changed value:\")\n",
    "print(W_center[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ULLmdXvC-QpX",
   "metadata": {
    "id": "ULLmdXvC-QpX"
   },
   "source": [
    "\n",
    "Write a training script for the GloVe model that goes over the entire co-occurrence matrix given a number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12dd535b",
   "metadata": {
    "id": "12dd535b"
   },
   "outputs": [],
   "source": [
    "def train_GloVe(matrix, vocab_size, epochs = 10, learning_rate = 0.0001, x_max = 10, hidden_dim=100):\n",
    "    \"\"\"\n",
    "    Train the glove model based the co-ocurrance matrix for a number of epochs\n",
    "    matrix: co-occcurance matrix\n",
    "    vocab_size: number of words in vocab\n",
    "    epochs: number of passes through the data\n",
    "    learning_rate: learning rate for back prop\n",
    "    x_max: parameter of the weighting function\n",
    "    hidden_dim: dimension of the vectors\n",
    "    \"\"\"\n",
    "    W_center, b_center, W_context, b_context = init_weights(vocab_size, hidden_dim)\n",
    "    for i in tqdm(range(epochs)):\n",
    "        ### perform backprop###\n",
    "        W_center, b_center, W_context, b_context, global_cost = back_prop(W_center, b_center, W_context, b_context, matrix, x_max, vocab_size, learning_rate)\n",
    "        print(f\"Global Cost: {global_cost}\")\n",
    "    return W_center, W_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0116c62",
   "metadata": {
    "id": "e0116c62"
   },
   "source": [
    "Train the model with hidden dimension of `100` and learning rate of `0.001` for a `100` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2355c76",
   "metadata": {
    "id": "b2355c76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                      | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [178868.39564671]\n",
      "Global Cost: [100199.52513265]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▊             | 6/100 [00:00<00:06, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [62095.85547419]\n",
      "Global Cost: [40984.08353369]\n",
      "Global Cost: [28325.43304851]\n",
      "Global Cost: [20308.86204787]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█             | 8/100 [00:00<00:06, 14.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [15015.3600184]\n",
      "Global Cost: [11398.70669579]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▌           | 12/100 [00:00<00:06, 14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [8854.97305717]\n",
      "Global Cost: [7019.89730915]\n",
      "Global Cost: [5665.83908265]\n",
      "Global Cost: [4646.23365818]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▊           | 14/100 [00:00<00:05, 14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [3864.25057017]\n",
      "Global Cost: [3254.43929527]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██▎          | 18/100 [00:01<00:05, 14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [2771.64012032]\n",
      "Global Cost: [2384.10264419]\n",
      "Global Cost: [2069.11406007]\n",
      "Global Cost: [1810.16324168]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██▌          | 20/100 [00:01<00:05, 14.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [1595.06603943]\n",
      "Global Cost: [1414.7043989]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▊          | 22/100 [00:01<00:05, 14.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [1262.1646679]\n",
      "Global Cost: [1132.13987264]\n",
      "Global Cost: [1020.50925754]\n",
      "Global Cost: [924.03858566]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███▍         | 26/100 [00:01<00:05, 14.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [840.16382471]\n",
      "Global Cost: [766.83314997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███▉         | 30/100 [00:02<00:04, 14.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [702.39022886]\n",
      "Global Cost: [645.48706719]\n",
      "Global Cost: [595.01825919]\n",
      "Global Cost: [550.070899]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████▏        | 32/100 [00:02<00:04, 14.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [509.88606796]\n",
      "Global Cost: [473.82896142]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████▋        | 36/100 [00:02<00:04, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [441.36552298]\n",
      "Global Cost: [412.04402429]\n",
      "Global Cost: [385.48043487]\n",
      "Global Cost: [361.34672018]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████▉        | 38/100 [00:02<00:04, 13.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [339.36141932]\n",
      "Global Cost: [319.28201061]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████▍       | 42/100 [00:02<00:04, 14.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [300.898689]\n",
      "Global Cost: [284.02926568]\n",
      "Global Cost: [268.51496548]\n",
      "Global Cost: [254.21694652]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████▏      | 48/100 [00:03<00:03, 14.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [241.01340445]\n",
      "Global Cost: [228.79715198]\n",
      "Global Cost: [217.47358742]\n",
      "Global Cost: [206.95898236]\n",
      "Global Cost: [197.17903311]\n",
      "Global Cost: [188.06763055]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████      | 54/100 [00:03<00:03, 13.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [179.56581185]\n",
      "Global Cost: [171.62086412]\n",
      "Global Cost: [164.18555553]\n",
      "Global Cost: [157.2174737]\n",
      "Global Cost: [150.67845474]\n",
      "Global Cost: [144.53408901]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████▊     | 60/100 [00:04<00:02, 14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [138.75329222]\n",
      "Global Cost: [133.30793217]\n",
      "Global Cost: [128.17250314]\n",
      "Global Cost: [123.32384106]\n",
      "Global Cost: [118.74087382]\n",
      "Global Cost: [114.40440185]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████▎    | 64/100 [00:04<00:02, 14.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [110.29690488]\n",
      "Global Cost: [106.40237143]\n",
      "Global Cost: [102.70614787]\n",
      "Global Cost: [99.19480482]\n",
      "Global Cost: [95.85601834]\n",
      "Global Cost: [92.67846438]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████    | 70/100 [00:04<00:02, 14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [89.65172458]\n",
      "Global Cost: [86.7662022]\n",
      "Global Cost: [84.01304693]\n",
      "Global Cost: [81.38408752]\n",
      "Global Cost: [78.87177129]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████▌   | 74/100 [00:05<00:01, 14.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [76.46910976]\n",
      "Global Cost: [74.16962975]\n",
      "Global Cost: [71.96732924]\n",
      "Global Cost: [69.85663753]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████▏  | 78/100 [00:05<00:01, 14.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [67.83237924]\n",
      "Global Cost: [65.88974169]\n",
      "Global Cost: [64.02424533]\n",
      "Global Cost: [62.23171694]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████▋  | 82/100 [00:05<00:01, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [60.50826521]\n",
      "Global Cost: [58.85025856]\n",
      "Global Cost: [57.25430492]\n",
      "Global Cost: [55.71723333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████▋ | 90/100 [00:06<00:00, 14.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [54.23607706]\n",
      "Global Cost: [52.80805824]\n",
      "Global Cost: [51.43057382]\n",
      "Global Cost: [50.10118261]\n",
      "Global Cost: [48.81759355]\n",
      "Global Cost: [47.57765481]\n",
      "Global Cost: [46.37934382]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████▋| 98/100 [00:06<00:00, 14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [45.22075817]\n",
      "Global Cost: [44.10010715]\n",
      "Global Cost: [43.015704]\n",
      "Global Cost: [41.96595878]\n",
      "Global Cost: [40.94937174]\n",
      "Global Cost: [39.96452727]\n",
      "Global Cost: [39.01008827]\n",
      "Global Cost: [38.08479096]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████| 100/100 [00:06<00:00, 14.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Cost: [37.18744002]\n",
      "Global Cost: [36.31690423]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "W_center, W_context = train_GloVe(matrix, vocab_size, epochs=100, learning_rate=0.001, hidden_dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770701f4",
   "metadata": {
    "id": "770701f4"
   },
   "source": [
    "As you can see by looking at the loss, the model still needs more time to converge to a minimum.\n",
    "However, we keep the training short and keeping in mind that the vectors can improve we look at some examples.\n",
    "Take the average, transpose, and normalize the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b272ab8a",
   "metadata": {
    "id": "b272ab8a"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "# take the average of the learned vector as the final vector\n",
    "W = np.add(W_center, W_context)/2\n",
    "W = W.T\n",
    "W = W/norm(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "334aff38",
   "metadata": {
    "id": "334aff38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 15818)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106c990",
   "metadata": {
    "id": "d106c990"
   },
   "source": [
    "Lets create a dictionary that points from a word to its vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db81b66e",
   "metadata": {
    "id": "db81b66e"
   },
   "outputs": [],
   "source": [
    "# Generates word to word embedding dictionary\n",
    "word_to_vector = {}\n",
    "for word in word_to_id.keys():\n",
    "    word_to_vector[word] = W[:, word_to_id[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c86eeb",
   "metadata": {
    "id": "11c86eeb"
   },
   "source": [
    "### Subtask 4: Compare to Skip-gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd8763",
   "metadata": {
    "id": "e8fd8763"
   },
   "source": [
    "Let's compute the similarities for the same words in Task 1 to compare the results with word2vec. This time you need to implement the similarity function, based on the dot product. To get to the topk you need to sort the elements based on their similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2f3f722",
   "metadata": {
    "id": "a2f3f722"
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "\n",
    "def most_similar(word_vector,all_vectors,id_to_word, topk):\n",
    "    \"\"\"\n",
    "    function to find the topk most similar words to a word vector\n",
    "    word_vector: vector of the search word\n",
    "    all_vectors: all word vectors\n",
    "    id_to_word: dictionary from id to words\n",
    "    topk: number of elements to return\n",
    "    \"\"\"\n",
    "    ### find the topk most similar words to a given word vector ##\n",
    "    similar_words = []\n",
    "    for index, vector in enumerate(all_vectors.T):\n",
    "        similarity = dot(word_vector, vector)\n",
    "        similar_words.append((id_to_word[index], similarity))\n",
    "    similar_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    word = similar_words[:topk]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53c1a84a",
   "metadata": {
    "id": "53c1a84a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('central_perk', 5.9981475413881216e-05),\n",
       " ('wisconsin', 2.6927178916588405e-05),\n",
       " ('whiffler', 2.3880521144401933e-05),\n",
       " ('minx', 2.3693300258652484e-05),\n",
       " ('bubby', 2.2186005794469956e-05)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(word_to_vector[\"central_perk\"],W,id_to_word,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c8e8519",
   "metadata": {
    "id": "2c8e8519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('joey', 5.379798526776008e-05),\n",
       " ('thumbs', 2.722126590354147e-05),\n",
       " ('enlightened', 2.223041702093662e-05),\n",
       " ('starts_leave', 2.1413345844736282e-05),\n",
       " ('hooked', 2.1267311725907695e-05)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(word_to_vector[\"joey\"],W,id_to_word,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcd5f5",
   "metadata": {
    "id": "6bdcd5f5"
   },
   "source": [
    "Compute the similarity between the `('rachel', 'mrs_green')`, `('smelly_cat', 'song')` and `('ross', 'spaceship')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e77f01a5",
   "metadata": {
    "id": "e77f01a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.1106483184860246e-06\n"
     ]
    }
   ],
   "source": [
    "similarity = dot(word_to_vector['rachel'], word_to_vector['mrs_green'])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1def847",
   "metadata": {
    "id": "d1def847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.887891268734376e-06\n"
     ]
    }
   ],
   "source": [
    "similarity = dot(word_to_vector['smelly_cat'], word_to_vector['song'])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "933ca3ac",
   "metadata": {
    "id": "933ca3ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.3388689300222487e-07\n"
     ]
    }
   ],
   "source": [
    "similarity = dot(word_to_vector['ross'], word_to_vector['spaceship'])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecace919",
   "metadata": {
    "id": "ecace919"
   },
   "source": [
    "If you see your results are not as meaningful as the gensim model, do not be discouraged. With better optimization and longer training the results should improve. If you have time play around a bit more with your model and see if you can generate more meaningful vectors."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
