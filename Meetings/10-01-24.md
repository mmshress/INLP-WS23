# 10-01-24

## New Approach

@sid-code14 @KushalGaywala
Updated from step 3.

- No need to fine-tune a model
- Now, can use RAG model

Components of vanilla RAG model

1. Vectorize the documents
2. Store in a vector store
3. Find top-k by vectorizing the query
4. Send the query with system prompt and similar documents to LLM
5. Get a new answer from LLM
6. How to evaluate?

## Decided Model

@sid-code14 @KushalGaywala @mmshress @asmaM1
We decided to use LLama 2 , as it is open source and free to use. when used LLama for demo, it provides good results, and if given good prompt and context, it produce acceptable results. For embedding, using E5-mistral-7b-instruct, which is top ranked on massive text embedding benchmark.
